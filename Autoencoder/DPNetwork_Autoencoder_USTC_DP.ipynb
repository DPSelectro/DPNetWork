{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DPNetwork-Autoencoder-USTC-DP.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYlxQL-1cOz4",
        "colab_type": "code",
        "outputId": "41eab463-7ea7-4f5e-ba37-4b57cbf1b159",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# This file trains the USTC using differential privacy \n",
        "# by adding nosie to autoencoder first layer \n",
        "# It uses a dense autoencoder. This file enables training \n",
        "# with both l1, l2 noise and both laplacian and gaussian \n",
        "# noise. See the https://arxiv.org/pdf/1802.03471.pdf for \n",
        "# details about how this works.\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Bc6CBVrcbCP",
        "colab_type": "code",
        "outputId": "a134e9cb-ba78-4343-9929-901c3608b7f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "# Move to drive\n",
        "%cd drive\n",
        "%cd 'My Drive'\n",
        "%cd Oxford-Thesis"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive\n",
            "/content/drive/My Drive\n",
            "/content/drive/My Drive/Oxford-Thesis\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pesGCHF7e1FF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import six \n",
        "import math\n",
        "from tensorflow.python.training import moving_averages \n",
        "import pickle\n",
        "import tensorflow.keras.backend as K\n",
        "from sklearn.preprocessing import normalize\n",
        "import tensorflow_probability as tfp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VP0wr4mCcrQd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Constants for deciding how to add differential privacy to autoencoder \n",
        "NOISE_PLACEMENT = 'img_noise'\n",
        "NOISE_SCHEME = 'l1_l1'\n",
        "FILTER_SIZES = 1\n",
        "NUM_FILTERS = 1 \n",
        "STRIDES = 1 \n",
        "EPSILON = 1.0\n",
        "DELTA_DP = 0.05\n",
        "SCALE = 0.1\n",
        "NOISE = 0\n",
        "BATCH_SHAPE = 128\n",
        "NOISE_TYPE = 'laplace'\n",
        "AUTO_ENCODER_OUT_SHAPE = 158\n",
        "CONT_FEATURE_SIZE = 158\n",
        "FOLDER = 'USTC2-AutoEncoderLaplace'\n",
        "NUM_CLASS =10\n",
        "TRAIN =True \n",
        "SCALE_STRING = \"0point1\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHJNSA-acjyD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## DP-Mult for difference \n",
        "def dp_mult( size=None):\n",
        "  epsilon_dp =EPSILON\n",
        "  delta_dp   = DELTA_DP\n",
        "  max_pixeldp_norm = SCALE\n",
        "  if NOISE_SCHEME == 'l1_l2'    or  \\\n",
        "      NOISE_SCHEME == 'l2':\n",
        "      return max_pixeldp_norm * \\\n",
        "          math.sqrt(2 * math.log(1.25 / delta_dp)) / epsilon_dp\n",
        "  elif NOISE_SCHEME == 'l1_l1'  or  \\\n",
        "      NOISE_SCHEME == 'l1':\n",
        "      return max_pixeldp_norm / epsilon_dp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5eOoT0sclC4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Create different types of noises for different \n",
        "## noise types and schemes\n",
        "def img_dp_noise(shape):\n",
        "  if NOISE_TYPE == 'laplace':\n",
        "    sensitivity_multiplier = EPSILON \n",
        "    dp_mult_x = dp_mult()\n",
        "    loc = tf.zeros([BATCH_SHAPE,shape])\n",
        "    scale = tf.ones([BATCH_SHAPE,shape])\n",
        "    epsilon = tfp.distributions.Laplace(loc,scale).sample()\n",
        "    noise_scale = tf.placeholder(tf.float32,shape =(), name ='noise_scale')\n",
        "    L1_SENSITIVITY = 1.0\n",
        "    b = L1_SENSITIVITY *dp_mult_x*SCALE\n",
        "    noise = b *epsilon\n",
        "    \n",
        "  elif NOISE_TYPE == 'gaussian':\n",
        "    dp_mult_x = dp_mult()\n",
        "    epsilon = tf.random_normal(shape=(BATCH_SHAPE,shape), mean=0,stddev=1)\n",
        "    sensitivity_multiplier = EPSILON \n",
        "    L2_SENSITIVITY = 1.0\n",
        "    noise_scale = tf.placeholder(tf.float32,shape =(), name ='noise_scale')\n",
        "    SIGMA = tf.multiply(dp_mult_x,L2_SENSITIVITY,name= \"SIGMA\")\n",
        "    noise_stddev = SCALE * SIGMA\n",
        "    noise = noise_stddev *epsilon\n",
        "  return noise "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GaeWsv0cmNJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Normalize the l1 direction https://github.com/columbia/pixeldp\n",
        "def l1_normalize(x, dim, epsilon=1e-12, name=None):\n",
        "  \"\"\"Normalizes along dimension `dim` using an L1 norm.\n",
        "  For a 1-D tensor with `dim = 0`, computes\n",
        "      output = x / max(sum(abs(x)), epsilon)\n",
        "  For `x` with more dimensions, independently normalizes each 1-D slice along\n",
        "  dimension `dim`.\n",
        "  Args:\n",
        "    x: A `Tensor`.\n",
        "    dim: Dimension along which to normalize.  A scalar or a vector of\n",
        "      integers.\n",
        "    epsilon: A lower bound value for the norm. Will use `sqrt(epsilon)` as the\n",
        "      divisor if `norm < sqrt(epsilon)`.\n",
        "    name: A name for this operation (optional).\n",
        "  Returns:\n",
        "    A `Tensor` with the same shape as `x`.\n",
        "  \"\"\"\n",
        "  with tf.name_scope(name, \"l1_normalize\", [x]) as name:\n",
        "    with tf.device(\"/gpu:0\"):\n",
        "      x          = tf.convert_to_tensor(x, name            = \"x\")\n",
        "      abs_sum    = tf.reduce_sum(tf.abs(x), dim, keep_dims = True)\n",
        "      x_inv_norm = tf.reciprocal(tf.maximum(abs_sum, epsilon))\n",
        "      return tf.multiply(x, x_inv_norm, name=name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqzxucIAcnsP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.device('/device:GPU:0'):\n",
        "  NOISE = img_dp_noise(int(CONT_FEATURE_SIZE/2))\n",
        "  ## Define the auto encoder without keras \n",
        "  x_ = tf.placeholder(tf.float32,shape= [BATCH_SHAPE,CONT_FEATURE_SIZE])\n",
        "  y_ =tf.placeholder(tf.float32,shape= [BATCH_SHAPE,CONT_FEATURE_SIZE])\n",
        "  \n",
        "  x2_ =tf.placeholder(tf.float32,shape= [BATCH_SHAPE,AUTO_ENCODER_OUT_SHAPE])\n",
        "  y2_ =tf.placeholder(tf.float32,shape= [BATCH_SHAPE,2])\n",
        "  y3_ =tf.placeholder(tf.float32,shape= [BATCH_SHAPE,10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MBcctJ5c5Vf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Autoencoder representation. (Allow different types of representations ie mid-based)\n",
        "def get_rep(x, full_rep = True):\n",
        "  with tf.variable_scope(\"first_model\",reuse=tf.AUTO_REUSE):\n",
        "    with tf.device('/device:GPU:0'):\n",
        "    \n",
        "      ## ADD NOISE \n",
        "      #noise_image = tf.add(NOISE,x) \n",
        "      W_dense1 = tf.get_variable(\"dense1\",initializer =tf.random_normal(stddev=0.1, shape =[CONT_FEATURE_SIZE,int(CONT_FEATURE_SIZE/2)]))\n",
        "      b_dense1 =  tf.get_variable(\"bias1\",initializer =tf.random_normal(stddev=0.1, shape =[int(CONT_FEATURE_SIZE/2)]))\n",
        "      if NOISE_SCHEME == \"l1_l2\":\n",
        "        W_dense1_norm = tf.nn.l2_normalize(W_dense1,dim=1)\n",
        "      else:\n",
        "        W_dense1_norm = l1_normalize(W_dense1,dim=1)\n",
        "\n",
        "      h_fc1 = tf.nn.relu(tf.matmul(tf.cast(x,tf.float32), W_dense1_norm))\n",
        "      if TRAIN == True:\n",
        "        noise_image = tf.add(h_fc1,NOISE)\n",
        "      else:\n",
        "        noise_image = tf.add(h_fc1,img_dp_noise(int(CONT_FEATURE_SIZE/2)))\n",
        "      \n",
        "      W_dense2 =tf.get_variable(\"dense2\",initializer =tf.random_normal(stddev=0.1,shape= [int(CONT_FEATURE_SIZE/2),int(CONT_FEATURE_SIZE/4)]))\n",
        "      b_dense2 = tf.get_variable(\"bias2\",initializer =tf.random_normal(stddev=0.1,shape= [int(CONT_FEATURE_SIZE/4)]))\n",
        "      h_fc2 = tf.nn.relu(tf.matmul(noise_image, W_dense2) + b_dense2)\n",
        "\n",
        "\n",
        "      W_dense3 =tf.get_variable(\"dense3\",initializer =tf.random_normal(stddev=0.1,shape= [int(CONT_FEATURE_SIZE/4),10]))\n",
        "      b_dense3 =tf.get_variable(\"bias3\",initializer =tf.random_normal(stddev=0.1,shape= [10])) \n",
        "      first_output = tf.matmul(h_fc2, W_dense3) + b_dense3\n",
        "      h_fc3 = tf.nn.relu(first_output)\n",
        "\n",
        "      W_dense4 =tf.get_variable(\"dense4\",initializer =tf.random_normal(stddev=0.1,shape= [10,int(CONT_FEATURE_SIZE/4)]))\n",
        "      b_dense4 =tf.get_variable(\"bias4\",initializer =tf.random_normal(stddev=0.1,shape= [int(CONT_FEATURE_SIZE/4)]))\n",
        "      h_fc4 = tf.nn.relu(tf.matmul(h_fc3, W_dense4) + b_dense4)\n",
        "\n",
        "      W_dense5 = tf.get_variable(\"dense5\",initializer =tf.random_normal(stddev=0.1,shape= [int(CONT_FEATURE_SIZE/4),int(CONT_FEATURE_SIZE/2)]))\n",
        "      b_dense5 = tf.get_variable(\"bias5\",initializer =tf.random_normal(stddev=0.1,shape= [int(CONT_FEATURE_SIZE/2)]))\n",
        "      h_fc5 = tf.nn.relu(tf.matmul(h_fc4, W_dense5) + b_dense5)\n",
        "\n",
        "      W_dense6 = tf.get_variable(\"dense6\",initializer =tf.random_normal(stddev=0.1,shape= [int(CONT_FEATURE_SIZE/2),int(CONT_FEATURE_SIZE)]))\n",
        "      b_dense6 = tf.get_variable(\"bias6\",initializer =tf.random_normal(stddev=0.1,shape= [CONT_FEATURE_SIZE]))\n",
        "      h_fc6 = tf.matmul(h_fc5, W_dense6) + b_dense6\n",
        "      if full_rep == True:\n",
        "        return h_fc6\n",
        "      return first_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQ-Vu_9KdGMB",
        "colab_type": "code",
        "outputId": "ba5932a6-411e-4036-b6a1-e923dbd40f39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "source": [
        "with tf.device('/device:GPU:0'):\n",
        "  mse = tf.reduce_mean(tf.losses.mean_squared_error(labels=y_, predictions=get_rep(x_,True)))\n",
        "  train_step = tf.train.AdamOptimizer(1e-4,name=\"adam-encoder\").minimize(mse)\n",
        "  accuracy = mse"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0807 23:38:56.392404 139640568006528 deprecation.py:506] From <ipython-input-9-ae2b94e09e25>:20: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n",
            "W0807 23:38:56.513452 139640568006528 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/losses/losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xkfVUL8dKUp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "## Load in data \n",
        "\n",
        "## Load in malicious training data\n",
        "with open('./embedding/CNN_FULL/USTC_rn_mal_meta_train', 'rb') as fp:\n",
        "  mal_train_meta_X = pickle.load(fp)\n",
        "  fp.close()\n",
        "  \n",
        "## Load in malicious validation data\n",
        "with open('./embedding/CNN_FULL/USTC_rn_mal_meta_valid', 'rb') as fp:\n",
        "  mal_valid_meta_X = pickle.load(fp)\n",
        "  fp.close()\n",
        "\n",
        "## Load in benign traning data \n",
        "with open('./embedding/CNN_FULL/USTC_rn_benign_meta_train', 'rb') as fp:\n",
        "  benign_train_meta_X = pickle.load(fp)\n",
        "  benign_train_meta_X = np.array(benign_train_meta_X)\n",
        "  fp.close()\n",
        "\n",
        "## Load in benign valid data \n",
        "with open('./embedding/CNN_FULL/USTC_rn_benign_meta_valid', 'rb') as fp:\n",
        "  benign_valid_meta_X = pickle.load(fp)\n",
        "  benign_valid_meta_X = np.array(benign_valid_meta_X)\n",
        "  fp.close()\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6N-HmlKeIo76",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels = [[0,1]]*len(benign_train_meta_X) +[[1,0]]*len(mal_train_meta_X)\n",
        "valid_labels = [[0,1]]*len(benign_valid_meta_X) +[[1,0]]*len(mal_valid_meta_X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUTD-NjDG1ok",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "## Randomize training and validation data \n",
        "train_meta_X = np.concatenate([benign_train_meta_X,mal_train_meta_X])\n",
        "valid_meta_X = np.concatenate([benign_valid_meta_X,mal_valid_meta_X])\n",
        "c = list(zip(train_meta_X,train_labels))\n",
        "np.random.shuffle(c)\n",
        "train_meta_X,train_labels = zip(*c)\n",
        "c = list(zip(valid_meta_X,valid_labels))\n",
        "np.random.shuffle(c)\n",
        "valid_meta_X,valid_labels = zip(*c)\n",
        "valid_meta_X = valid_meta_X[:53242]\n",
        "train_meta_X = train_meta_X[:534400]\n",
        "train_X =np.array(train_meta_X).astype(float)\n",
        "valid_X =np.array(valid_meta_X).astype(float)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USQlrF9QgAD9",
        "colab_type": "code",
        "outputId": "ce79f63f-bd64-4531-ddd2-ae0ff1d10037",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "saver = tf.train.Saver()\n",
        "config = tf.ConfigProto(allow_soft_placement=True)\n",
        "config.gpu_options.allow_growth = True\n",
        "print(\"GPU Available: \", tf.test.is_gpu_available())\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU Available:  True\n",
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRY6751OffNt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Get nosie for training\n",
        "session = tf.Session(config=config)\n",
        "session.run(tf.global_variables_initializer())\n",
        "with session as sess:\n",
        "  noise = sess.run(NOISE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iS6XD81rggKQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Dump noise for later use and training the CNN later \n",
        "with open('./DP-GRAPHS/' + FOLDER +'/NOISE'+SCALE_STRING, 'wb') as fp:\n",
        "  pickle.dump(noise,fp)\n",
        "  fp.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cHwyMCAhfZ_k",
        "outputId": "4b980323-da2d-4b90-fcf5-12548241b3e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "## Train the autoencoder\n",
        "EPOCHS = 60\n",
        "mses = []\n",
        "with session as sess:\n",
        "  for epoch in range(EPOCHS):\n",
        "    for batch_num in range(int(len(train_X)/BATCH_SHAPE)):\n",
        "      batch = train_X[batch_num*BATCH_SHAPE:batch_num*BATCH_SHAPE+BATCH_SHAPE]\n",
        "      batch_label = train_X[batch_num*BATCH_SHAPE:batch_num*BATCH_SHAPE+BATCH_SHAPE]\n",
        "      train_accuracy += accuracy.eval(feed_dict={x_:batch, y_: batch_label})\n",
        "      train_step.run(feed_dict={x_: batch, y_: batch_label})\n",
        "    mses.append(train_accuracy/(int(len(train_X)/BATCH_SHAPE)))\n",
        "    print('Step %d, training accuracy %g' % (batch_num , train_accuracy/(int(len(train_X)/BATCH_SHAPE))))\n",
        "    valid_acc = 0 \n",
        "    for batch_num in range(int(len(valid_X)/BATCH_SHAPE)): \n",
        "      batch = valid_X[batch_num*BATCH_SHAPE:batch_num*BATCH_SHAPE+BATCH_SHAPE]\n",
        "      batch_label = valid_X[batch_num*BATCH_SHAPE:batch_num*BATCH_SHAPE+BATCH_SHAPE]\n",
        "      valid_acc += accuracy.eval(feed_dict={x_:batch, y_: batch_label})\n",
        "    print('Step %d, validation accuracy %g' % (batch_num , valid_acc/(int(len(valid_X)/BATCH_SHAPE)))) \n",
        "  saver.save(sess, './DP-GRAPHS/' + FOLDER+ '/autoencoder'+SCALE_STRING+'.ckpt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 4174, training accuracy 0.0221551\n",
            "Step 414, validation accuracy 0.0122127\n",
            "Step 4174, training accuracy 0.0106217\n",
            "Step 414, validation accuracy 0.00789582\n",
            "Step 4174, training accuracy 0.00756857\n",
            "Step 414, validation accuracy 0.00649442\n",
            "Step 4174, training accuracy 0.00638501\n",
            "Step 414, validation accuracy 0.00581085\n",
            "Step 4174, training accuracy 0.00599744\n",
            "Step 414, validation accuracy 0.0055023\n",
            "Step 4174, training accuracy 0.00578557\n",
            "Step 414, validation accuracy 0.00515908\n",
            "Step 4174, training accuracy 0.00559602\n",
            "Step 414, validation accuracy 0.00502722\n",
            "Step 4174, training accuracy 0.00545079\n",
            "Step 414, validation accuracy 0.00479661\n",
            "Step 4174, training accuracy 0.00534846\n",
            "Step 414, validation accuracy 0.0046358\n",
            "Step 4174, training accuracy 0.00524375\n",
            "Step 414, validation accuracy 0.00458593\n",
            "Step 4174, training accuracy 0.00519219\n",
            "Step 414, validation accuracy 0.00455081\n",
            "Step 4174, training accuracy 0.00512401\n",
            "Step 414, validation accuracy 0.00456781\n",
            "Step 4174, training accuracy 0.00507657\n",
            "Step 414, validation accuracy 0.00456778\n",
            "Step 4174, training accuracy 0.00503118\n",
            "Step 414, validation accuracy 0.00453951\n",
            "Step 4174, training accuracy 0.00500584\n",
            "Step 414, validation accuracy 0.0043996\n",
            "Step 4174, training accuracy 0.00496023\n",
            "Step 414, validation accuracy 0.00447553\n",
            "Step 4174, training accuracy 0.00492806\n",
            "Step 414, validation accuracy 0.00441085\n",
            "Step 4174, training accuracy 0.00491302\n",
            "Step 414, validation accuracy 0.00433203\n",
            "Step 4174, training accuracy 0.00488876\n",
            "Step 414, validation accuracy 0.00440713\n",
            "Step 4174, training accuracy 0.00486693\n",
            "Step 414, validation accuracy 0.00434736\n",
            "Step 4174, training accuracy 0.0048493\n",
            "Step 414, validation accuracy 0.0043809\n",
            "Step 4174, training accuracy 0.00482405\n",
            "Step 414, validation accuracy 0.00434261\n",
            "Step 4174, training accuracy 0.00480941\n",
            "Step 414, validation accuracy 0.00439171\n",
            "Step 4174, training accuracy 0.00479828\n",
            "Step 414, validation accuracy 0.00433713\n",
            "Step 4174, training accuracy 0.00477988\n",
            "Step 414, validation accuracy 0.00434021\n",
            "Step 4174, training accuracy 0.00476361\n",
            "Step 414, validation accuracy 0.0043133\n",
            "Step 4174, training accuracy 0.00475922\n",
            "Step 414, validation accuracy 0.00434146\n",
            "Step 4174, training accuracy 0.00475247\n",
            "Step 414, validation accuracy 0.00428772\n",
            "Step 4174, training accuracy 0.00472791\n",
            "Step 414, validation accuracy 0.00432264\n",
            "Step 4174, training accuracy 0.00473128\n",
            "Step 414, validation accuracy 0.0043161\n",
            "Step 4174, training accuracy 0.00472583\n",
            "Step 414, validation accuracy 0.00427766\n",
            "Step 4174, training accuracy 0.00472556\n",
            "Step 414, validation accuracy 0.00424937\n",
            "Step 4174, training accuracy 0.00470102\n",
            "Step 414, validation accuracy 0.00427109\n",
            "Step 4174, training accuracy 0.00469674\n",
            "Step 414, validation accuracy 0.00433108\n",
            "Step 4174, training accuracy 0.00468026\n",
            "Step 414, validation accuracy 0.00426214\n",
            "Step 4174, training accuracy 0.00469336\n",
            "Step 414, validation accuracy 0.00424747\n",
            "Step 4174, training accuracy 0.00468727\n",
            "Step 414, validation accuracy 0.00425684\n",
            "Step 4174, training accuracy 0.00468269\n",
            "Step 414, validation accuracy 0.00424585\n",
            "Step 4174, training accuracy 0.00467798\n",
            "Step 414, validation accuracy 0.0042556\n",
            "Step 4174, training accuracy 0.0046589\n",
            "Step 414, validation accuracy 0.00423933\n",
            "Step 4174, training accuracy 0.00467525\n",
            "Step 414, validation accuracy 0.00428364\n",
            "Step 4174, training accuracy 0.00466936\n",
            "Step 414, validation accuracy 0.00429131\n",
            "Step 4174, training accuracy 0.00465997\n",
            "Step 414, validation accuracy 0.00426303\n",
            "Step 4174, training accuracy 0.0046497\n",
            "Step 414, validation accuracy 0.0042911\n",
            "Step 4174, training accuracy 0.00465338\n",
            "Step 414, validation accuracy 0.0042512\n",
            "Step 4174, training accuracy 0.00464318\n",
            "Step 414, validation accuracy 0.004219\n",
            "Step 4174, training accuracy 0.00465548\n",
            "Step 414, validation accuracy 0.0042907\n",
            "Step 4174, training accuracy 0.00462519\n",
            "Step 414, validation accuracy 0.00423517\n",
            "Step 4174, training accuracy 0.00463501\n",
            "Step 414, validation accuracy 0.00425585\n",
            "Step 4174, training accuracy 0.00462291\n",
            "Step 414, validation accuracy 0.00425549\n",
            "Step 4174, training accuracy 0.00463981\n",
            "Step 414, validation accuracy 0.0042907\n",
            "Step 4174, training accuracy 0.00463611\n",
            "Step 414, validation accuracy 0.00424372\n",
            "Step 4174, training accuracy 0.00461681\n",
            "Step 414, validation accuracy 0.00416251\n",
            "Step 4174, training accuracy 0.00462089\n",
            "Step 414, validation accuracy 0.00429636\n",
            "Step 4174, training accuracy 0.00461933\n",
            "Step 414, validation accuracy 0.00426488\n",
            "Step 4174, training accuracy 0.00461859\n",
            "Step 414, validation accuracy 0.0042459\n",
            "Step 4174, training accuracy 0.00461719\n",
            "Step 414, validation accuracy 0.00426039\n",
            "Step 4174, training accuracy 0.00461737\n",
            "Step 414, validation accuracy 0.00419517\n",
            "Step 4174, training accuracy 0.00461137\n",
            "Step 414, validation accuracy 0.00424378\n",
            "Step 4174, training accuracy 0.00459023\n",
            "Step 414, validation accuracy 0.00428671\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rocUZquyfbbC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}