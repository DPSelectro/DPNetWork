{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DPNetwork-Autoencoder-UNSW-DP.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "m93udc3S4s2R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This file trains the USTC using differential privacy \n",
        "# by adding nosie to autoencoder first layer \n",
        "# It uses a dense autoencoder. This file enables training \n",
        "# with both l1, l2 noise and both laplacian and gaussian \n",
        "# noise. See the https://arxiv.org/pdf/1802.03471.pdf for \n",
        "# details about how this works.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdVy41d_4-bC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Move to drive\n",
        "%cd drive\n",
        "%cd 'My Drive'\n",
        "%cd Oxford-Thesis"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkTyTRos57Ez",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import six \n",
        "import math\n",
        "from tensorflow.python.training import moving_averages \n",
        "import pickle\n",
        "import tensorflow.keras.backend as K\n",
        "from sklearn.preprocessing import normalize\n",
        "import tensorflow_probability as tfp\n",
        "CONT_FEATURE_SIZE = 38\n",
        "NUM_BENIGN = 187426\n",
        "NUM_SCAN = 2073\n",
        "NUM_FUZZ = 1087\n",
        "NUM_ANALY = 558\n",
        "NUM_BACK = 92\n",
        "NUM_DOS = 1709\n",
        "NUM_EXP = 11481\n",
        "NUM_GEN = 1699\n",
        "NUM_SHELL = 0\n",
        "NUM_WORM = 148"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9AmQFVh3aSR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Constants for deciding how to add differential privacy to autoencoder \n",
        "NOISE_PLACEMENT = 'img_noise'\n",
        "NOISE_SCHEME = 'l1_l1'\n",
        "FILTER_SIZES = 1\n",
        "NUM_FILTERS = 1 \n",
        "STRIDES = 1 \n",
        "EPSILON = 1.0\n",
        "DELTA_DP = 0.05\n",
        "SCALE = 0.1\n",
        "NOISE = 0\n",
        "BATCH_SHAPE = 128\n",
        "NOISE_TYPE = 'laplace'\n",
        "AUTO_ENCODER_OUT_SHAPE = 158\n",
        "CONT_FEATURE_SIZE = 158\n",
        "FOLDER = 'UNSW-AutoEncoderLaplace'\n",
        "NUM_CLASS =10\n",
        "TRAIN =True \n",
        "SCALE_STRING = \"0point1\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7cp64sq16rE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Load in data \n",
        "\n",
        "## Load in malicious training data\n",
        "with open('./embedding/CNN_FULL/UNSW_mal_meta_train', 'rb') as fp:\n",
        "  mal_train_meta_X = pickle.load(fp)\n",
        "  fp.close()\n",
        "  \n",
        "## Load in malicious validation data\n",
        "with open('./embedding/CNN_FULL/UNSW_mal_meta_valid', 'rb') as fp:\n",
        "  mal_valid_meta_X = pickle.load(fp)\n",
        "  fp.close()\n",
        "  \n",
        "## Load in benign traning data  \n",
        "with open('./embedding/CNN_FULL/UNSW_train_benign_meta', 'rb') as fp:\n",
        "  benign_train_meta_X =pickle.load(fp)\n",
        "  fp.close()\n",
        "  \n",
        "## Load in benign valid data\n",
        "with open('./embedding/CNN_FULL/UNSW_valid_benign_meta', 'rb') as fp:\n",
        "  benign_valid_meta_X = pickle.load(fp)\n",
        "  fp.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCKAkWYf3J5H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels = [[0,1]]*len(benign_train_meta_X) +[[1,0]]*len(mal_train_meta_X)\n",
        "valid_labels = [[0,1]]*len(benign_valid_meta_X) +[[1,0]]*len(mal_valid_meta_X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXsAZ1pS28r8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Randomize training and validation data \n",
        "train_X = np.concatenate([benign_train_meta_X,mal_train_meta_X])\n",
        "print(train_X.shape)\n",
        "c = list(zip(list(train_X),train_labels))\n",
        "np.random.shuffle(c)\n",
        "train_X, train_labels = zip(*c)\n",
        "valid_X = np.concatenate([benign_valid_meta_X,mal_valid_meta_X])\n",
        "c = list(zip(list(valid_X),valid_labels))\n",
        "np.random.shuffle(c)\n",
        "valid_X, valid_labels = zip(*c)\n",
        "\n",
        "train_X = np.array(train_X)\n",
        "valid_X = np.array(valid_X)\n",
        "train_X = train_X.astype(float)\n",
        "valid_X = valid_X.astype(float)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKRUSURbBDFF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## DP-Mult for different types of noise \n",
        "def dp_mult( size=None):\n",
        "  epsilon_dp =EPSILON\n",
        "  delta_dp   = DELTA_DP\n",
        "  max_pixeldp_norm = SCALE\n",
        "  if NOISE_SCHEME == 'l1_l2'    or  \\\n",
        "      NOISE_SCHEME == 'l2':\n",
        "      return max_pixeldp_norm * \\\n",
        "          math.sqrt(2 * math.log(1.25 / delta_dp)) / epsilon_dp\n",
        "  elif NOISE_SCHEME == 'l1_l1'  or  \\\n",
        "      NOISE_SCHEME == 'l1':\n",
        "      return max_pixeldp_norm / epsilon_dp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dq7YLxC0BEWd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "## Create different types of noises for different \n",
        "## noise types and schemes\n",
        "def img_dp_noise(shape):\n",
        "  if NOISE_TYPE == 'laplace':\n",
        "    sensitivity_multiplier = EPSILON \n",
        "    dp_mult_x = dp_mult()\n",
        "    loc = tf.zeros([BATCH_SHAPE,shape])\n",
        "    scale = tf.ones([BATCH_SHAPE,shape])\n",
        "    epsilon = tfp.distributions.Laplace(loc,scale).sample()\n",
        "    noise_scale = tf.placeholder(tf.float32,shape =(), name ='noise_scale')\n",
        "    L1_SENSITIVITY = 1.0\n",
        "    b = L1_SENSITIVITY *dp_mult_x*SCALE\n",
        "    noise = b *epsilon\n",
        "    \n",
        "  elif NOISE_TYPE == 'gaussian':\n",
        "    dp_mult_x = dp_mult()\n",
        "    epsilon = tf.random_normal(shape=(BATCH_SHAPE,shape), mean=0,stddev=1)\n",
        "    sensitivity_multiplier = EPSILON \n",
        "    L2_SENSITIVITY = 1.0\n",
        "    noise_scale = tf.placeholder(tf.float32,shape =(), name ='noise_scale')\n",
        "    SIGMA = tf.multiply(dp_mult_x,L2_SENSITIVITY,name= \"SIGMA\")\n",
        "    noise_stddev = SCALE * SIGMA\n",
        "    noise = noise_stddev *epsilon\n",
        "  return noise"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nhJLpHXBFgN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Normalize the l1 direction https://github.com/columbia/pixeldp\n",
        "def l1_normalize(x, dim, epsilon=1e-12, name=None):\n",
        "  \"\"\"Normalizes along dimension `dim` using an L1 norm.\n",
        "  For a 1-D tensor with `dim = 0`, computes\n",
        "      output = x / max(sum(abs(x)), epsilon)\n",
        "  For `x` with more dimensions, independently normalizes each 1-D slice along\n",
        "  dimension `dim`.\n",
        "  Args:\n",
        "    x: A `Tensor`.\n",
        "    dim: Dimension along which to normalize.  A scalar or a vector of\n",
        "      integers.\n",
        "    epsilon: A lower bound value for the norm. Will use `sqrt(epsilon)` as the\n",
        "      divisor if `norm < sqrt(epsilon)`.\n",
        "    name: A name for this operation (optional).\n",
        "  Returns:\n",
        "    A `Tensor` with the same shape as `x`.\n",
        "  \"\"\"\n",
        "  with tf.name_scope(name, \"l1_normalize\", [x]) as name:\n",
        "    with tf.device(\"/gpu:0\"):\n",
        "      x          = tf.convert_to_tensor(x, name            = \"x\")\n",
        "      abs_sum    = tf.reduce_sum(tf.abs(x), dim, keep_dims = True)\n",
        "      x_inv_norm = tf.reciprocal(tf.maximum(abs_sum, epsilon))\n",
        "      return tf.multiply(x, x_inv_norm, name=name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nB1S67HVBG9S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.device('/device:GPU:0'):\n",
        "  NOISE = img_dp_noise(int(CONT_FEATURE_SIZE/2))\n",
        "  ## Define the auto encoder without keras \n",
        "  x_ = tf.placeholder(tf.float32,shape= [BATCH_SHAPE,CONT_FEATURE_SIZE])\n",
        "  y_ =tf.placeholder(tf.float32,shape= [BATCH_SHAPE,CONT_FEATURE_SIZE])\n",
        "  \n",
        "  x2_ =tf.placeholder(tf.float32,shape= [BATCH_SHAPE,AUTO_ENCODER_OUT_SHAPE])\n",
        "  y2_ =tf.placeholder(tf.float32,shape= [BATCH_SHAPE,2])\n",
        "  y3_ =tf.placeholder(tf.float32,shape= [BATCH_SHAPE,10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwg64U_rBIES",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_rep(x, full_rep = True):\n",
        "  with tf.variable_scope(\"first_model\",reuse=tf.AUTO_REUSE):\n",
        "    with tf.device('/device:GPU:0'):\n",
        "    \n",
        "      ## ADD NOISE \n",
        "      #noise_image = tf.add(NOISE,x) \n",
        "      W_dense1 = tf.get_variable(\"dense1\",initializer =tf.random_normal(stddev=0.1, shape =[CONT_FEATURE_SIZE,int(CONT_FEATURE_SIZE/2)]))\n",
        "      b_dense1 =  tf.get_variable(\"bias1\",initializer =tf.random_normal(stddev=0.1, shape =[int(CONT_FEATURE_SIZE/2)]))\n",
        "      if NOISE_SCHEME == \"l1_l2\":\n",
        "        W_dense1_norm = tf.nn.l2_normalize(W_dense1,dim=1)\n",
        "      else:\n",
        "        W_dense1_norm = l1_normalize(W_dense1,dim=1)\n",
        "\n",
        "      h_fc1 = tf.nn.relu(tf.matmul(tf.cast(x,tf.float32), W_dense1_norm))\n",
        "      if TRAIN == True:\n",
        "        noise_image = tf.add(h_fc1,NOISE)\n",
        "      else:\n",
        "        noise_image = tf.add(h_fc1,img_dp_noise(int(CONT_FEATURE_SIZE/2)))\n",
        "      \n",
        "      W_dense2 =tf.get_variable(\"dense2\",initializer =tf.random_normal(stddev=0.1,shape= [int(CONT_FEATURE_SIZE/2),int(CONT_FEATURE_SIZE/4)]))\n",
        "      b_dense2 = tf.get_variable(\"bias2\",initializer =tf.random_normal(stddev=0.1,shape= [int(CONT_FEATURE_SIZE/4)]))\n",
        "      h_fc2 = tf.nn.relu(tf.matmul(noise_image, W_dense2) + b_dense2)\n",
        "\n",
        "\n",
        "      W_dense3 =tf.get_variable(\"dense3\",initializer =tf.random_normal(stddev=0.1,shape= [int(CONT_FEATURE_SIZE/4),10]))\n",
        "      b_dense3 =tf.get_variable(\"bias3\",initializer =tf.random_normal(stddev=0.1,shape= [10])) \n",
        "      first_output = tf.matmul(h_fc2, W_dense3) + b_dense3\n",
        "      h_fc3 = tf.nn.relu(first_output)\n",
        "\n",
        "      W_dense4 =tf.get_variable(\"dense4\",initializer =tf.random_normal(stddev=0.1,shape= [10,int(CONT_FEATURE_SIZE/4)]))\n",
        "      b_dense4 =tf.get_variable(\"bias4\",initializer =tf.random_normal(stddev=0.1,shape= [int(CONT_FEATURE_SIZE/4)]))\n",
        "      h_fc4 = tf.nn.relu(tf.matmul(h_fc3, W_dense4) + b_dense4)\n",
        "\n",
        "      W_dense5 = tf.get_variable(\"dense5\",initializer =tf.random_normal(stddev=0.1,shape= [int(CONT_FEATURE_SIZE/4),int(CONT_FEATURE_SIZE/2)]))\n",
        "      b_dense5 = tf.get_variable(\"bias5\",initializer =tf.random_normal(stddev=0.1,shape= [int(CONT_FEATURE_SIZE/2)]))\n",
        "      h_fc5 = tf.nn.relu(tf.matmul(h_fc4, W_dense5) + b_dense5)\n",
        "\n",
        "      W_dense6 = tf.get_variable(\"dense6\",initializer =tf.random_normal(stddev=0.1,shape= [int(CONT_FEATURE_SIZE/2),int(CONT_FEATURE_SIZE)]))\n",
        "      b_dense6 = tf.get_variable(\"bias6\",initializer =tf.random_normal(stddev=0.1,shape= [CONT_FEATURE_SIZE]))\n",
        "      h_fc6 = tf.matmul(h_fc5, W_dense6) + b_dense6\n",
        "      if full_rep == True:\n",
        "        return h_fc6\n",
        "      return first_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wemZh3aeBKEm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_logits_cnn(x,return_embedding= False):\n",
        "  with  tf.variable_scope(\"tree_second_model\",reuse=tf.AUTO_REUSE):\n",
        "    with  tf.device('/device:GPU:0'):\n",
        "      W_dense4_class = tf.get_variable(\"dense4_sec\",initializer =tf.random_normal(stddev=0.1, shape =[AUTO_ENCODER_OUT_SHAPE,784] ))\n",
        "      b_dense4_class = tf.get_variable(\"bias4_sec\",initializer =tf.random_normal(stddev=0.1,shape=[784]))\n",
        "      h_fc4_class = tf.nn.relu(tf.matmul(tf.cast(x,tf.float32), W_dense4_class) + b_dense4_class)\n",
        "\n",
        "      reshape_1_class = tf.reshape(h_fc4_class,[BATCH_SHAPE,28,28,1])\n",
        "      conv2d_1_weight = tf.get_variable(\"conv2d_1_dense\",initializer =tf.random_normal(stddev=0.1, shape =[5, 5, 1, 32]))\n",
        "      conv2d_1_bias = tf.get_variable(\"conv2d_1_bias\",initializer =tf.random_normal(stddev=0.1, shape= [32]))\n",
        "      conv2d_1_class = tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(reshape_1_class,\n",
        "                                                  conv2d_1_weight,\n",
        "                                                  strides =[1,1,1,1],\n",
        "                                                  padding='SAME'),\n",
        "                                      conv2d_1_bias))\n",
        "\n",
        "      maxpool2d_1_class = tf.nn.max_pool(conv2d_1_class,\n",
        "                                       ksize=[1, 2, 2, 1], \n",
        "                                       strides=[1, 2, 2, 1],\n",
        "                                padding='SAME')\n",
        "      conv2d_2_weight = tf.get_variable(\"conv2d_2_dense\",initializer =tf.random_normal(stddev=0.1, shape =[5, 5, 32, 64]))\n",
        "      conv2d_2_bias = tf.get_variable(\"conv2d_2_bias\",initializer =tf.random_normal(stddev=0.1, shape=[64]))\n",
        "      conv2d_2_class = tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(maxpool2d_1_class,\n",
        "                                                  conv2d_2_weight,\n",
        "                                                  strides =[1,1,1,1],\n",
        "                                                  padding='SAME'),\n",
        "                                      conv2d_2_bias))\n",
        "\n",
        "      maxpool2d_2_class = tf.nn.max_pool(conv2d_2_class,\n",
        "                                       ksize=[1, 2, 2, 1], \n",
        "                                       strides=[1, 2, 2, 1],\n",
        "                                padding='SAME')\n",
        "\n",
        "      reshape_2_class = tf.reshape(maxpool2d_2_class,[BATCH_SHAPE,-1])\n",
        "      W_dense5_class = tf.get_variable(\"dense5_sec\",initializer =tf.random_normal(stddev=0.1,shape=[7*7*64,1024]))\n",
        "      b_dense5_class = tf.get_variable(\"bias5_sec\",initializer =tf.random_normal(stddev=0.1,shape=[1024]))\n",
        "      h_fc5_logit = tf.nn.bias_add(tf.matmul(reshape_2_class, W_dense5_class),b_dense5_class)\n",
        "      h_fc5_class = tf.nn.relu(h_fc5_logit)\n",
        "\n",
        "      out_weight_class =  tf.get_variable(\"out_dense_sec\",initializer =tf.random_normal(stddev=0.1,shape=[1024, 2]))\n",
        "      out_bias_class =  tf.get_variable(\"out_bias_sec\",initializer =tf.random_normal(stddev=0.1,shape = [2]))\n",
        "      out_final_logits = tf.nn.bias_add(tf.matmul(h_fc5_class,out_weight_class),out_bias_class)\n",
        "      if return_embedding == True:\n",
        "        return h_fc5_logit\n",
        "      return out_final_logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSyN_C6A2s_A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.device('/device:GPU:0'):\n",
        "\n",
        "  mse = tf.reduce_mean(tf.losses.mean_squared_error(labels=y_, predictions=get_rep(x_,True)))\n",
        "  train_step = tf.train.AdamOptimizer(1e-4,name=\"adam-encoder\").minimize(mse)\n",
        "  accuracy = mse"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4Hiy_VI2wqg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = tf.ConfigProto(allow_soft_placement=True)\n",
        "config.gpu_options.allow_growth = True\n",
        "print(\"GPU Available: \", tf.test.is_gpu_available())\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0wPgIB92yiR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_mqAiag3NFF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "session = tf.Session(config=config)\n",
        "session.run(tf.global_variables_initializer())\n",
        "with session as sess:\n",
        "  noise = sess.run(NOISE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAYhCkoS3OLI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Dump noise for later use and training the CNN later \n",
        "with open('./DP-GRAPHS/' + FOLDER +'/NOISE'+SCALE_STRING, 'wb') as fp:\n",
        "  pickle.dump(noise,fp)\n",
        "  fp.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17P38HCo3Ro4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Train the autoencoder\n",
        "EPOCHS = 60\n",
        "mses = []\n",
        "with session as sess:\n",
        "  for epoch in range(EPOCHS):\n",
        "    for batch_num in range(int(len(train_X)/BATCH_SHAPE)):\n",
        "      batch = train_X[batch_num*BATCH_SHAPE:batch_num*BATCH_SHAPE+BATCH_SHAPE]\n",
        "      batch_label = train_X[batch_num*BATCH_SHAPE:batch_num*BATCH_SHAPE+BATCH_SHAPE]\n",
        "      train_accuracy += accuracy.eval(feed_dict={x_:batch, y_: batch_label})\n",
        "      train_step.run(feed_dict={x_: batch, y_: batch_label})\n",
        "    mses.append(train_accuracy/(int(len(train_X)/BATCH_SHAPE)))\n",
        "    print('Step %d, training accuracy %g' % (batch_num , train_accuracy/(int(len(train_X)/BATCH_SHAPE))))\n",
        "    valid_acc = 0 \n",
        "    for batch_num in range(int(len(valid_X)/BATCH_SHAPE)): \n",
        "      batch = valid_X[batch_num*BATCH_SHAPE:batch_num*BATCH_SHAPE+BATCH_SHAPE]\n",
        "      batch_label = valid_X[batch_num*BATCH_SHAPE:batch_num*BATCH_SHAPE+BATCH_SHAPE]\n",
        "      valid_acc += accuracy.eval(feed_dict={x_:batch, y_: batch_label})\n",
        "    print('Step %d, validation accuracy %g' % (batch_num , valid_acc/(int(len(valid_X)/BATCH_SHAPE)))) \n",
        "  saver.save(sess, './DP-GRAPHS/' + FOLDER+ '/autoencoder'+SCALE_STRING+'.ckpt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rB6rmsYo-Dds",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}