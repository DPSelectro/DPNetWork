{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DPNetwork-BlackBox-AdvExample-USTC2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUzUW4QUUV7I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This file performs a black box adversrial attack on network used to differetiate malicious \n",
        "# from benign flows. It uses Sparse1D attacks to attack the network. In order to generate\n",
        "# new examples for training it uses a Jacobian graph  \n",
        "\n",
        "# Initialize drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWFVrexgU11x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Move to drive \n",
        "%cd drive\n",
        "%cd 'My Drive'\n",
        "%cd Oxford-Thesis"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqWAs3WvX5iF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " import numpy as np\n",
        "import tensorflow as tf\n",
        "import six \n",
        "import math\n",
        "from tensorflow.python.training import moving_averages \n",
        "import pickle\n",
        "import tensorflow.keras.backend as K\n",
        "from sklearn.preprocessing import normalize\n",
        "import tensorflow_probability as tfp\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from __future__ import unicode_literals\n",
        "import functools\n",
        "import logging\n",
        "import numpy as np\n",
        "from six.moves import xrange\n",
        "import tensorflow as tf\n",
        "CONT_FEATURE_SIZE = 158"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTKSD_vde_Kz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install git+https://github.com/tensorflow/cleverhans.git#egg=cleverhans"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoXVdn-BXwmR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from cleverhans.attacks import FastGradientMethod\n",
        "from cleverhans.utils_tf import jacobian_graph, jacobian_augmentation\n",
        "from cleverhans.compat import flags\n",
        "from cleverhans.initializers import HeReLuNormalInitializer\n",
        "from cleverhans.attacks import SparseL1Descent  \n",
        "from cleverhans.loss import CrossEntropy\n",
        "from cleverhans.model import Model\n",
        "from cleverhans.train import train\n",
        "from cleverhans.utils import set_log_level\n",
        "from cleverhans.utils import TemporaryLogLevel\n",
        "from cleverhans.utils import to_categorical\n",
        "from cleverhans.utils_tf import model_eval\n",
        "from cleverhans.evaluation import batch_eval\n",
        "import pickle\n",
        "import cleverhans.model as chm\n",
        "import cleverhans as ch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S663QD5Sas6D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Constants for creating the black box substitute model \n",
        "NB_CLASSES = 2\n",
        "LEARNING_RATE = .001\n",
        "NB_EPOCHS = 10\n",
        "HOLDOUT = 150\n",
        "DATA_AUG = 6\n",
        "NB_EPOCHS_S = 10\n",
        "LMBDA = .1\n",
        "AUG_BATCH_SIZE = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36bSImKO7X3P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Load in data for training the substitute model \n",
        "with open('./embedding/CNN_FULL/USTC_rn_mal_meta_valid', 'rb') as fp:\n",
        "  mal_valid_meta_X = pickle.load(fp)\n",
        "  fp.close()\n",
        "\n",
        "with open('./embedding/CNN_FULL/USTC_rn_benign_meta_valid', 'rb') as fp:\n",
        "  benign_valid_meta_X = pickle.load(fp)\n",
        "  benign_valid_meta_X = np.array(benign_valid_meta_X)\n",
        "  fp.close()\n",
        "valid_labels = [[0,1]]*len(benign_valid_meta_X) +[[1,0]]*len(mal_valid_meta_X)\n",
        "\n",
        "import numpy as np\n",
        "valid_labels = [[0,1]]*len(benign_valid_meta_X) +[[1,0]]*len(mal_valid_meta_X)\n",
        "valid_meta_X = np.concatenate([benign_valid_meta_X,mal_valid_meta_X])\n",
        "c = list(zip(valid_meta_X,valid_labels))\n",
        "np.random.shuffle(c)\n",
        "valid_meta_X,valid_labels = zip(*c)\n",
        "valid_X =np.array(valid_meta_X).astype(float)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oy3iZVJNVDlc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## CNN model for differentiating the malicious and bengin data\n",
        "def get_logits_cnn(x,return_embedding= False):\n",
        "  with  tf.variable_scope(\"tree_second_model\",reuse=tf.AUTO_REUSE):\n",
        "    with  tf.device('/device:GPU:0'):\n",
        "      W_dense4_class = tf.get_variable(\"dense4_sec\",initializer =tf.random_normal(stddev=0.1, shape =[158,784] ))\n",
        "      b_dense4_class = tf.get_variable(\"bias4_sec\",initializer =tf.random_normal(stddev=0.1,shape=[784]))\n",
        "      h_fc4_class = tf.nn.relu(tf.matmul(tf.cast(x,tf.float32), W_dense4_class) + b_dense4_class)\n",
        "\n",
        "      reshape_1_class = tf.reshape(h_fc4_class,[BATCH_SHAPE,28,28,1])\n",
        "      conv2d_1_weight = tf.get_variable(\"conv2d_1_dense\",initializer =tf.random_normal(stddev=0.1, shape =[5, 5, 1, 32]))\n",
        "      conv2d_1_bias = tf.get_variable(\"conv2d_1_bias\",initializer =tf.random_normal(stddev=0.1, shape= [32]))\n",
        "      conv2d_1_class = tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(reshape_1_class,\n",
        "                                                  conv2d_1_weight,\n",
        "                                                  strides =[1,1,1,1],\n",
        "                                                  padding='SAME'),\n",
        "                                      conv2d_1_bias))\n",
        "\n",
        "      maxpool2d_1_class = tf.nn.max_pool(conv2d_1_class,\n",
        "                                       ksize=[1, 2, 2, 1], \n",
        "                                       strides=[1, 2, 2, 1],\n",
        "                                padding='SAME')\n",
        "      conv2d_2_weight = tf.get_variable(\"conv2d_2_dense\",initializer =tf.random_normal(stddev=0.1, shape =[5, 5, 32, 64]))\n",
        "      conv2d_2_bias = tf.get_variable(\"conv2d_2_bias\",initializer =tf.random_normal(stddev=0.1, shape=[64]))\n",
        "      conv2d_2_class = tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(maxpool2d_1_class,\n",
        "                                                  conv2d_2_weight,\n",
        "                                                  strides =[1,1,1,1],\n",
        "                                                  padding='SAME'),\n",
        "                                      conv2d_2_bias))\n",
        "\n",
        "      maxpool2d_2_class = tf.nn.max_pool(conv2d_2_class,\n",
        "                                       ksize=[1, 2, 2, 1], \n",
        "                                       strides=[1, 2, 2, 1],\n",
        "                                padding='SAME')\n",
        "\n",
        "      reshape_2_class = tf.reshape(maxpool2d_2_class,[BATCH_SHAPE,-1])\n",
        "      drop_one = tf.nn.dropout(reshape_2_class,rate=0.2)\n",
        "      W_dense5_class = tf.get_variable(\"dense5_sec\",initializer =tf.random_normal(stddev=0.1,shape=[7*7*64,1024]))\n",
        "      b_dense5_class = tf.get_variable(\"bias5_sec\",initializer =tf.random_normal(stddev=0.1,shape=[1024]))\n",
        "      h_fc5_logit = tf.nn.bias_add(tf.matmul(drop_one, W_dense5_class),b_dense5_class)\n",
        "      h_fc5_class = tf.nn.relu(h_fc5_logit)\n",
        "      drop_two = tf.nn.dropout(h_fc5_class,rate=0.4)\n",
        "\n",
        "      out_weight_class =  tf.get_variable(\"out_dense_sec\",initializer =tf.random_normal(stddev=0.1,shape=[1024, 2]))\n",
        "      out_bias_class =  tf.get_variable(\"out_bias_sec\",initializer =tf.random_normal(stddev=0.1,shape = [2]))\n",
        "      out_final_logits = tf.nn.bias_add(tf.matmul(drop_two,out_weight_class),out_bias_class)\n",
        "      if return_embedding == True:\n",
        "        return h_fc5_logit\n",
        "      return out_final_logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXl_vLgGVHha",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_logits(x):\n",
        "  return get_logits_cnn(get_rep(x, full_rep=True ))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyJQ1NmzVN7b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Define the placeholders for training the cnn models \n",
        " with tf.device('/device:GPU:0'):\n",
        "  x2_ = tf.placeholder(tf.float32,shape= [BATCH_SHAPE,CONT_FEATURE_SIZE])\n",
        "  y2_ =tf.placeholder(tf.float32,shape= [BATCH_SHAPE,2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9if_gH4XVPf7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = tf.ConfigProto(allow_soft_placement=True)\n",
        "config.gpu_options.allow_growth = True\n",
        "print(\"GPU Available: \", tf.test.is_gpu_available())\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pD0DO4EiVRLJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Tensors for training the cnn model \n",
        "with tf.device('/device:GPU:0'):\n",
        "  cnn_cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y2_, logits=get_logits_cnn(x2_)))\n",
        "  cnn_class_train_step = tf.train.AdamOptimizer(1e-4,name=\"class-adam-encoder\").minimize(cnn_cross_entropy)\n",
        "  cnn_correct_prediction = tf.equal(tf.argmax(get_logits_cnn(x2_), 1), tf.argmax(y2_, 1))\n",
        "  cnn_accuracy = tf.reduce_mean(tf.cast(cnn_correct_prediction, tf.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WA9GiWRBVW7-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Saver for saving and reloading the original cnn model and the black box model \n",
        "second_saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNzwz9DTXAtJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Sub logits for retraining the model after getting the substitute data\n",
        "def get_next_sub_logits(x,return_embedding= False):\n",
        "  with  tf.variable_scope(\"sub1\",reuse=tf.AUTO_REUSE):\n",
        "    with tf.device('/device:GPU:0'):\n",
        "      W_dense4_class = tf.get_variable(\"dense4_sec\",initializer =tf.random_normal(stddev=0.1, shape =[AUTO_ENCODER_OUT_SHAPE,784] ))\n",
        "      b_dense4_class = tf.get_variable(\"bias4_sec\",initializer =tf.random_normal(stddev=0.1,shape=[784]))\n",
        "      h_fc4_class = tf.nn.relu(tf.matmul(tf.cast(x,tf.float32), W_dense4_class) + b_dense4_class)\n",
        "\n",
        "      reshape_1_class = tf.reshape(h_fc4_class,[BATCH_SHAPE,28,28,1])\n",
        "      conv2d_1_weight = tf.get_variable(\"conv2d_1_dense\",initializer =tf.random_normal(stddev=0.1, shape =[5, 5, 1, 32]))\n",
        "      conv2d_1_bias = tf.get_variable(\"conv2d_1_bias\",initializer =tf.random_normal(stddev=0.1, shape= [32]))\n",
        "      conv2d_1_class = tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(reshape_1_class,\n",
        "                                                  conv2d_1_weight,\n",
        "                                                  strides =[1,1,1,1],\n",
        "                                                  padding='SAME'),\n",
        "                                      conv2d_1_bias))\n",
        "\n",
        "      maxpool2d_1_class = tf.nn.max_pool(conv2d_1_class,\n",
        "                                       ksize=[1, 2, 2, 1], \n",
        "                                       strides=[1, 2, 2, 1],\n",
        "                                padding='SAME')\n",
        "      conv2d_2_weight = tf.get_variable(\"conv2d_2_dense\",initializer =tf.random_normal(stddev=0.1, shape =[5, 5, 32, 64]))\n",
        "      conv2d_2_bias = tf.get_variable(\"conv2d_2_bias\",initializer =tf.random_normal(stddev=0.1, shape=[64]))\n",
        "      conv2d_2_class = tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(maxpool2d_1_class,\n",
        "                                                  conv2d_2_weight,\n",
        "                                                  strides =[1,1,1,1],\n",
        "                                                  padding='SAME'),\n",
        "                                      conv2d_2_bias))\n",
        "\n",
        "      maxpool2d_2_class = tf.nn.max_pool(conv2d_2_class,\n",
        "                                       ksize=[1, 2, 2, 1], \n",
        "                                       strides=[1, 2, 2, 1],\n",
        "                                padding='SAME')\n",
        "\n",
        "      reshape_2_class = tf.reshape(maxpool2d_2_class,[BATCH_SHAPE,-1])\n",
        "      drop_one = tf.nn.dropout(reshape_2_class,rate=0.2)\n",
        "      W_dense5_class = tf.get_variable(\"dense5_sec\",initializer =tf.random_normal(stddev=0.1,shape=[7*7*64,1024]))\n",
        "      b_dense5_class = tf.get_variable(\"bias5_sec\",initializer =tf.random_normal(stddev=0.1,shape=[1024]))\n",
        "      h_fc5_logit = tf.nn.bias_add(tf.matmul(drop_one, W_dense5_class),b_dense5_class)\n",
        "      h_fc5_class = tf.nn.relu(h_fc5_logit)\n",
        "      drop_two = tf.nn.dropout(h_fc5_class,rate=0.4)\n",
        "\n",
        "      out_weight_class =  tf.get_variable(\"out_dense_sec\",initializer =tf.random_normal(stddev=0.1,shape=[1024, 2]))\n",
        "      out_bias_class =  tf.get_variable(\"out_bias_sec\",initializer =tf.random_normal(stddev=0.1,shape = [2]))\n",
        "      out_final_logits = tf.nn.bias_add(tf.matmul(drop_two,out_weight_class),out_bias_class)\n",
        "      if return_embedding == True:\n",
        "        return h_fc5_logit\n",
        "      return out_final_logits\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dneyfgwXCf_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Sub logits for training the model while getting the substitute data\n",
        "def get_sub_logits(x,return_embedding= False):\n",
        "  with tf.device('/device:GPU:0'):\n",
        "      W_dense4_class = tf.get_variable(\"dense4_sec\",initializer =tf.random_normal(stddev=0.1, shape =[AUTO_ENCODER_OUT_SHAPE,784] ))\n",
        "      b_dense4_class = tf.get_variable(\"bias4_sec\",initializer =tf.random_normal(stddev=0.1,shape=[784]))\n",
        "      h_fc4_class = tf.nn.relu(tf.matmul(tf.cast(x,tf.float32), W_dense4_class) + b_dense4_class)\n",
        "\n",
        "      reshape_1_class = tf.reshape(h_fc4_class,[BATCH_SHAPE,28,28,1])\n",
        "      conv2d_1_weight = tf.get_variable(\"conv2d_1_dense\",initializer =tf.random_normal(stddev=0.1, shape =[5, 5, 1, 32]))\n",
        "      conv2d_1_bias = tf.get_variable(\"conv2d_1_bias\",initializer =tf.random_normal(stddev=0.1, shape= [32]))\n",
        "      conv2d_1_class = tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(reshape_1_class,\n",
        "                                                  conv2d_1_weight,\n",
        "                                                  strides =[1,1,1,1],\n",
        "                                                  padding='SAME'),\n",
        "                                      conv2d_1_bias))\n",
        "\n",
        "      maxpool2d_1_class = tf.nn.max_pool(conv2d_1_class,\n",
        "                                       ksize=[1, 2, 2, 1], \n",
        "                                       strides=[1, 2, 2, 1],\n",
        "                                padding='SAME')\n",
        "      conv2d_2_weight = tf.get_variable(\"conv2d_2_dense\",initializer =tf.random_normal(stddev=0.1, shape =[5, 5, 32, 64]))\n",
        "      conv2d_2_bias = tf.get_variable(\"conv2d_2_bias\",initializer =tf.random_normal(stddev=0.1, shape=[64]))\n",
        "      conv2d_2_class = tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(maxpool2d_1_class,\n",
        "                                                  conv2d_2_weight,\n",
        "                                                  strides =[1,1,1,1],\n",
        "                                                  padding='SAME'),\n",
        "                                      conv2d_2_bias))\n",
        "\n",
        "      maxpool2d_2_class = tf.nn.max_pool(conv2d_2_class,\n",
        "                                       ksize=[1, 2, 2, 1], \n",
        "                                       strides=[1, 2, 2, 1],\n",
        "                                padding='SAME')\n",
        "\n",
        "      reshape_2_class = tf.reshape(maxpool2d_2_class,[BATCH_SHAPE,-1])\n",
        "      drop_one = tf.nn.dropout(reshape_2_class,rate=0.2)\n",
        "      W_dense5_class = tf.get_variable(\"dense5_sec\",initializer =tf.random_normal(stddev=0.1,shape=[7*7*64,1024]))\n",
        "      b_dense5_class = tf.get_variable(\"bias5_sec\",initializer =tf.random_normal(stddev=0.1,shape=[1024]))\n",
        "      h_fc5_logit = tf.nn.bias_add(tf.matmul(drop_one, W_dense5_class),b_dense5_class)\n",
        "      h_fc5_class = tf.nn.relu(h_fc5_logit)\n",
        "      drop_two = tf.nn.dropout(h_fc5_class,rate=0.4)\n",
        "\n",
        "      out_weight_class =  tf.get_variable(\"out_dense_sec\",initializer =tf.random_normal(stddev=0.1,shape=[1024, 2]))\n",
        "      out_bias_class =  tf.get_variable(\"out_bias_sec\",initializer =tf.random_normal(stddev=0.1,shape = [2]))\n",
        "      out_final_logits = tf.nn.bias_add(tf.matmul(drop_two,out_weight_class),out_bias_class)\n",
        "      if return_embedding == True:\n",
        "        return h_fc5_logit\n",
        "      return out_final_logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnzTksEpXGNV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Variable scope for changing the scope while training the substitute model \n",
        "TOP_SCOPE = tf.get_variable_scope()\n",
        "MODEL_SCOPE = tf.get_variable_scope()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfUHrb3PACXZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Gets accuracy based on true and predicted logits\n",
        "def get_accuracy(true_logits,predicted_logits):\n",
        "  accuracy = 0 \n",
        "  for index in range(len(true_logits)):\n",
        "    true_predict = np.argmax(true_logits[index])\n",
        "    adv_predict = np.argmax(predicted_logits[index])\n",
        "    if true_predict == adv_predict:\n",
        "      accuracy +=1\n",
        "  return float(accuracy)/len(true_logits)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCiFHizrVhfv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Prepares the black box for training \n",
        "def prep_bbox(sess):\n",
        "  with  tf.device('/device:GPU:0'):\n",
        "    nn_model = chm.CallableModelWrapper(get_logits_cnn, 'logits')\n",
        "    print(\"Classification Network Benign vs Malicious \")\n",
        "    batch = valid_X[:BATCH_SHAPE]\n",
        "    print(len(batch))\n",
        "    non_adv_output = get_logits_cnn(tf.cast(np.array(batch),tf.float32))\n",
        "    valid_acc = 0\n",
        "    for batch_num in range(1,int(len(valid_X)/BATCH_SHAPE)): \n",
        "      print(batch_num)\n",
        "      batch = valid_X[batch_num*BATCH_SHAPE:batch_num*BATCH_SHAPE+BATCH_SHAPE]\n",
        "      batch_label = valid_labels[batch_num*BATCH_SHAPE:batch_num*BATCH_SHAPE+BATCH_SHAPE]\n",
        "      non_adv_out = get_logits_cnn(tf.cast(np.array(batch),tf.float32))\n",
        "      non_adv_output = tf.concat([non_adv_output,non_adv_out],axis=0)\n",
        "    valid_cnn_logits = sess.run(non_adv_output) \n",
        "    return nn_model, valid_cnn_logits, valid_acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZbrYrLiI7rX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Gets logits of the original model while training the substitute model \n",
        "def get_logits(sess,x):\n",
        "  with tf.variable_scope(TOP_SCOPE): \n",
        "    batch = x[:BATCH_SHAPE]\n",
        "    non_adv_output = get_logits_cnn(tf.cast(np.array(batch),tf.float32))\n",
        "    for batch_num in range(1,int(len(x)/BATCH_SHAPE)): \n",
        "      print(batch_num)\n",
        "      batch = valid_X[batch_num*BATCH_SHAPE:batch_num*BATCH_SHAPE+BATCH_SHAPE]\n",
        "      non_adv_out = get_logits_cnn(tf.cast(np.array(batch),tf.float32))\n",
        "      non_adv_output = tf.concat([non_adv_output,non_adv_out],axis=0)\n",
        "    return sess.run(non_adv_output)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNBBEgeUXoql",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Class for training cleverhans substitute model \n",
        "class ModelSubstitute(Model): \n",
        "  def __init__(self, scope, **kwargs):\n",
        "    del kwargs\n",
        "    Model.__init__(self, scope, nb_classes, locals())\n",
        "    self.nb_filters = nb_filters\n",
        "  \n",
        "  def fprop(self, x, **kwargs):\n",
        "    del kwargs\n",
        "    with tf.variable_scope('model_s',reuse=tf.AUTO_REUSE):\n",
        "      logits = get_sub_logits(x)\n",
        "      print(\"GET LOGITS\")\n",
        "    return {self.O_LOGITS: logits,\n",
        "            self.O_PROBS: tf.nn.softmax(logits=logits)}\n",
        "\n",
        "## Trains the cleverhans substitute model \n",
        "def train_sub(sess, x, y, bbox_preds, x_sub, y_sub, nb_classes,\n",
        "              nb_epochs_s, batch_size, learning_rate, data_aug, lmbda,\n",
        "              aug_batch_size, rng,nchannels=1):\n",
        "  with tf.variable_scope('model_s',reuse=tf.AUTO_REUSE):\n",
        "    model_sub = ModelSubstitute('model_s', nb_classes)\n",
        "    preds_sub = model_sub.get_logits(x)\n",
        "    loss_sub = CrossEntropy(model_sub, smoothing=0)\n",
        "    # Define the Jacobian symbolically using TensorFlow\n",
        "    grads = jacobian_graph(preds_sub, x, nb_classes)\n",
        "    # Train the substitute and augment dataset alternatively\n",
        "    for rho in xrange(data_aug):\n",
        "      print(\"Substitute training epoch #\" + str(rho))\n",
        "      train_params = {\n",
        "          'nb_epochs': nb_epochs_s,\n",
        "          'batch_size': batch_size,\n",
        "          'learning_rate': learning_rate\n",
        "      }\n",
        "      print(\"TRAINING\")\n",
        "      train(sess, loss_sub,x_sub.astype(float), to_categorical(y_sub, nb_classes),\n",
        "              init_all=False, args=train_params, rng=rng,\n",
        "              var_list=model_sub.get_params())\n",
        "      print(\"END_TRAINING\")\n",
        "      if rho < data_aug - 1:\n",
        "        print(\"Augmenting substitute training data.\")\n",
        "        lmbda_coef = 2 * int(int(rho / 3) != 0) - 1\n",
        "        x_sub = jacobian_augmentation(sess, x, x_sub, y_sub, grads,\n",
        "                                      lmbda_coef * lmbda, aug_batch_size)\n",
        "        print(\"Labeling substitute training data.\")\n",
        "        y_sub = np.hstack([y_sub, y_sub])\n",
        "        x_sub_prev = x_sub[int(len(x_sub)/2):]\n",
        "        eval_params = {'batch_size': batch_size}\n",
        "        # bbox_val = batch_eval(sess, [x], [tf.convert_to_tensor(bbox_preds[:256])],\n",
        "        #                       [x_sub_prev], batch_size=256)[0]\n",
        "        bbox_val = get_logits(sess,x_sub_prev)\n",
        "        print(len(bbox_val))               \n",
        "        # Note here that we take the argmax because the adversary\n",
        "        # only has access to the label (not the probabilities) output\n",
        "        # by the black-box model\n",
        "\n",
        "        y_sub[int(len(x_sub)/2):] = np.argmax(bbox_val, axis=1)\n",
        "    with open('./DP-GRAPHS/sub_x','wb') as fp:\n",
        "      pickle.dump(x_sub,fp)\n",
        "    with open('./DP-GRAPHS/sub_y','wb') as fp:\n",
        "      pickle.dump(y_sub,fp)\n",
        "    return model_sub, preds_sub\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZQ4aouOacA_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Defines and trains cnn blackbox model for generating the adversarial examples\n",
        "def cnn_blackbox(nb_classes=NB_CLASSES,\n",
        "                   batch_size=BATCH_SIZE, learning_rate=LEARNING_RATE,\n",
        "                   nb_epochs=NB_EPOCHS, holdout=HOLDOUT, data_aug=DATA_AUG,\n",
        "                   nb_epochs_s=NB_EPOCHS_S, lmbda=LMBDA,\n",
        "                   aug_batch_size=AUG_BATCH_SIZE):\n",
        "  # Dictionary used to keep track and return key accuracies\n",
        "  accuracies = {}\n",
        "  session = tf.Session()\n",
        "  with session as sess:\n",
        "    TOP_SCOPE = tf.get_variable_scope()\n",
        "    ## Restore the original model for getting logits\n",
        "    second_saver.restore(sess, './DP-GRAPHS/CNN_BASIC.ckpt')\n",
        "    x_sub = valid_X[:holdout]\n",
        "    y_sub = np.argmax(valid_labels[:holdout], axis=1)\n",
        "\n",
        "    x_test = valid_X[holdout:]\n",
        "    y_test = valid_labels[holdout:]\n",
        "\n",
        "    x = tf.placeholder(tf.float32, shape=(BATCH_SIZE, CONT_FEATURE_SIZE))\n",
        "    y = tf.placeholder(tf.float32, shape=(BATCH_SIZE, nb_classes))\n",
        "\n",
        "    rng = np.random.RandomState([2019, 8, 6])\n",
        "    print(\"Preparing the black-box model.\")\n",
        "    prep_bbox_out = prep_bbox(sess)\n",
        "    model, bbox_preds, accuracies['bbox'] = prep_bbox_out\n",
        "    MODEL = model\n",
        "    BBOX_PREDS= bbox_preds\n",
        "    ACCURACY =  accuracies['bbox']\n",
        "    print(\"Training the substitute model.\")\n",
        "    train_sub_out = train_sub(sess, x, y, bbox_preds, x_sub, y_sub,\n",
        "                              nb_classes, nb_epochs_s, batch_size,\n",
        "                              learning_rate, data_aug, lmbda, aug_batch_size,\n",
        "                              rng)\n",
        "    model_sub, preds_sub = train_sub_out\n",
        "    eval_params = {'batch_size': batch_size}\n",
        "    acc = model_eval(sess, x, y, preds_sub, np.array(x_test), np.array(y_test), args=eval_params)\n",
        "    accuracies['sub'] = acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tT-cQH_jdKeP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if CREATE:\n",
        "  cnn_blackbox(nb_classes=2,batch_size=128,learning_rate=0.001,nb_epochs=10,holdout=512,data_aug=6)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsOnYCc7gnZp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Reopen the synthetic substitute training model data\n",
        "with open('./DP-GRAPHS/sub_x','rb') as fp:\n",
        "  x_sub = pickle.load (fp)\n",
        "with open('./DP-GRAPHS/sub_y','rb') as fp:\n",
        "  y_sub = pickle.load(fp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rf94oJImk5or",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Train substitute Model\n",
        "with tf.device('/device:GPU:0'):\n",
        "  sub_cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y2_, logits=get_next_sub_logits(x2_)))\n",
        "  sub_class_train_step = tf.train.AdamOptimizer(1e-3,name=\"sub-adam-encoder\").minimize(sub_cross_entropy)\n",
        "  sub_correct_prediction = tf.equal(tf.argmax(get_next_sub_logits(x2_), 1), tf.argmax(y2_, 1))\n",
        "  sub_accuracy = tf.reduce_mean(tf.cast(sub_correct_prediction, tf.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFUAPSnpvhph",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Saver for reopening the substitute model \n",
        "saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Patv_QIz0rhJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Train the substitute model using the generated data\n",
        "if CREATE:\n",
        "  session = tf.Session(config=config)\n",
        "  session.run(tf.global_variables_initializer())\n",
        "  SIZE = 128\n",
        "  EPOCHS = 10\n",
        "  with session as sess:\n",
        "    second_saver.restore(sess, './DP-GRAPHS/CNN_BASIC.ckpt')\n",
        "    while SIZE < len(x_sub)*2:\n",
        "      batches = x_sub[:SIZE]\n",
        "      labels = y_labels[:SIZE]\n",
        "      EPOCHS = EPOCHS+10\n",
        "      for EPOCH in range(EPOCHS):\n",
        "        train_acc = 0\n",
        "        for batch_num in range(int(len(batches)/BATCH_SHAPE)):\n",
        "          batch = batches[batch_num*BATCH_SHAPE:batch_num*BATCH_SHAPE+BATCH_SHAPE]\n",
        "          batch_label = labels[batch_num*BATCH_SHAPE:batch_num*BATCH_SHAPE+BATCH_SHAPE]\n",
        "          current_train_acc = sub_accuracy.eval(feed_dict={x2_:batch, y2_: batch_label})\n",
        "          sub_class_train_step.run(feed_dict={x2_:batch, y2_: batch_label})\n",
        "          train_acc += current_train_acc\n",
        "        train_acc = float(train_acc)/(len(batches)/BATCH_SHAPE)\n",
        "        print(\"EPOCH: \" +str(EPOCH) +\" Accuracy: \"+ str(train_acc))\n",
        "      SIZE= SIZE*2\n",
        "    saver.save(sess, './DP-GRAPHS/USTC2-sub.ckpt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nI1zj8lu_NXz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Tensor for getting the CNN loits \n",
        "cnn_logits = get_logits_cnn(x2_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBJ9TdOxzNiY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Get valid malicious flows  ##\n",
        "index = 0 \n",
        "indices = []\n",
        "for label in valid_labels:\n",
        "  if label[0] == 1:\n",
        "    indices.append(index)\n",
        "  index+=1\n",
        "valid_malicious_flows = []\n",
        "for i in indices:\n",
        "  valid_malicious_flows.append(valid_X[i])\n",
        "print(len(valid_malicious_flows))\n",
        "\n",
        "## Get benign flows ##\n",
        "index = 0 \n",
        "for label in valid_labels:\n",
        "  if label[0] == 0:\n",
        "    indices.append(index)\n",
        "  index+=1\n",
        "valid_benign_flows = []\n",
        "for i in indices:\n",
        "  valid_benign_flows.append(valid_X[i])\n",
        "print(len(valid_benign_flows))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DtgXh_2xbL1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Get malicious flows for creating adversarial examples \n",
        "NUM_SAMPLED = 17664\n",
        "test_malicious_flows_index = np.random.choice(len(valid_malicious_flows), NUM_SAMPLED , replace=False)\n",
        "test_malicious_flows = []\n",
        "for index in test_malicious_flows_index:\n",
        "  test_malicious_flows.append(valid_malicious_flows[index])\n",
        "test_malicious_flows = np.array(test_malicious_flows).astype(float)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0rcDdPEvcEw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "session = tf.Session(config=config)\n",
        "session.run(tf.global_variables_initializer())\n",
        "accuracy =1\n",
        "with session as sess: \n",
        "\n",
        "  ## Restore the USTC substitute model \n",
        "  saver.restore(sess, './DP-GRAPHS/USTC2-sub.ckpt')\n",
        "  with  tf.device('/device:GPU:0'):\n",
        "    print(\"Classification Network Benign vs Malicious \")\n",
        "    valid_acc = 0 \n",
        "\n",
        "    ## Check the validation accuracy \n",
        "    for batch_num in range(int(len(valid_X)/BATCH_SHAPE)): \n",
        "      batch = valid_X[batch_num*BATCH_SHAPE:batch_num*BATCH_SHAPE+BATCH_SHAPE]\n",
        "      batch_label = valid_labels[batch_num*BATCH_SHAPE:batch_num*BATCH_SHAPE+BATCH_SHAPE]\n",
        "      valid_acc += cnn_accuracy.eval(feed_dict={x2_:batch, y2_: batch_label})\n",
        "    print('Step %d, validation accuracy %g' % (batch_num , valid_acc/(int(len(valid_X)/BATCH_SHAPE))))     \n",
        "    \n",
        "    ## Get the the callable cleverhans model for creating the \n",
        "    ## adversarial model \n",
        "    nn_model = chm.CallableModelWrapper(get_next_sub_logits, 'logits')\n",
        "    fgsm = SparseL1Descent(nn_model,sess =sess)\n",
        "    fgsm_params = {'eps_iter':0.005,\n",
        "            'eps': SCALE,\n",
        "            'nb_iter': 100,                   \n",
        "            'ord': 1,\n",
        "                        'clip_min': 0,\n",
        "                        'clip_max': 1.0,\n",
        "                      'y_target':tf.convert_to_tensor(np.array([[0,1]]*128))}\n",
        "    batch = test_malicious_flows[:BATCH_SHAPE]\n",
        "\n",
        "    ## Gernerate the adversarial example \n",
        "    adv_x = fgsm.generate(tf.cast(np.array(batch),tf.float32), **fgsm_params)\n",
        "\n",
        "    ## Ge al the adversrial examples\n",
        "    for batch_num in range(1,int(len(test_malicious_flows)/BATCH_SHAPE)):\n",
        "      print(batch_num)\n",
        "      batch =test_malicious_flows[batch_num*BATCH_SHAPE:batch_num*BATCH_SHAPE+BATCH_SHAPE]\n",
        "      batch_size = len(batch)\n",
        "      adv_output = fgsm.generate(tf.cast(np.array(batch),tf.float32), **fgsm_params)\n",
        "      adv_x  = tf.concat([adv_x, adv_output[:batch_size]],axis = 0)\n",
        "    adversarial_output = sess.run(adv_x)\n",
        "\n",
        "    print(\"GET SOFTMAX ADV EXAMPLES\")\n",
        "    ## Run to get softmax adversarial outputs ##\n",
        "    batch = adversarial_output[:BATCH_SHAPE]\n",
        "    adv_soft_output = tf.nn.softmax(cnn_logits.eval(feed_dict={x2_:batch}))\n",
        "    for batch_num in range(1,int(len(adversarial_output)/BATCH_SHAPE)):\n",
        "      print(batch_num)\n",
        "      batch = adversarial_output[batch_num*BATCH_SHAPE:batch_num*BATCH_SHAPE+BATCH_SHAPE]\n",
        "      adv_soft_out = tf.nn.softmax(cnn_logits.eval(feed_dict={x2_:batch}))\n",
        "      adv_soft_output = tf.concat([adv_soft_output,adv_soft_out],axis = 0) \n",
        "    adv_soft_output = sess.run(adv_soft_output) \n",
        "\n",
        "    ## Get the final accuracies \n",
        "    accuracy = get_accuracy([[1,0]]*128,adv_soft_output[:18176])\n",
        "    print('Test accuracy of oracle on adversarial examples generated '\n",
        "          'using the substitute: ' + str(accuracy))\n",
        "    print(accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OF1n4JEf3hVh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FOLDER = 'USTC2-BasicBlackBox'\n",
        "with open('./ADV-Example/' + FOLDER+ '/RESULTS'+SCALE_STRING+'.txt', 'w') as fp:\n",
        "  fp.write(\" Test accuracy of oracle on adversarial examples generated using the substitute: \" +str(accuracy))\n",
        "  fp.close()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}