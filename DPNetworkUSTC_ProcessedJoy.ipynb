{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DPNetworkUSTC-ProcessedJoy.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.6"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8BZGJc_dSTR3",
        "colab": {}
      },
      "source": [
        "## This file handles the processing fo Joy files. It retrieves flow metadata \n",
        "## as well as data concerning \n",
        "## DP Network 2019\n",
        "\n",
        "# Imports\n",
        "from __future__ import absolute_import, division, print_function\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import sys\n",
        "import pickle\n",
        "import scipy\n",
        "import pandas as pd\n",
        "import math\n",
        "import json\n",
        "import gzip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKKudO09XwHh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Validate the adresses in the flows are\n",
        "## IP4  adresses \n",
        "def validate_ip(s):\n",
        "    a = s.split('.')\n",
        "    if len(a) != 4:\n",
        "        return False\n",
        "    for x in a:\n",
        "        if not x.isdigit():\n",
        "            return False\n",
        "        i = int(x)\n",
        "        if i < 0 or i > 255:\n",
        "            return False\n",
        "    return True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "E0Ehuo6GSTR9",
        "colab": {}
      },
      "source": [
        "class DataParser:\n",
        "\n",
        "    def __init__(self, json_file, compact=1):\n",
        "        self.flows = []\n",
        "        self.compact = compact\n",
        "        with gzip.open(json_file, 'r') as fp:\n",
        "            try:\n",
        "                for line in fp:\n",
        "                    try:\n",
        "                        tmp = json.loads(line)\n",
        "                        if 'version' not in tmp:\n",
        "                            self.flows.append(tmp)\n",
        "                    except:\n",
        "                        continue\n",
        "            except:\n",
        "                return\n",
        "\n",
        "          \n",
        "    ## Get timing information for indiivudal flows\n",
        "    def getFlowSetMetadataTimings(self):\n",
        "        if self.flows == []:\n",
        "            return None\n",
        "        data = dict()\n",
        "        ## Go through each flowset\n",
        "        for flow in self.flows:\n",
        "            if len(flow['packets']) == 0:\n",
        "                continue\n",
        "            tmp = []\n",
        "            time_start = 0\n",
        "            if validate_ip(flow['sa']) == False or validate_ip(flow['da']) == False:\n",
        "              continue\n",
        "            ## Use appropriate flow key \n",
        "            key = flow['sa'].replace('.','')+flow['da'].replace('.','')+str(flow['sp'])+str(flow['dp'])+str(flow['pr'])               \n",
        "            if 'time_start' in flow:\n",
        "                time_start = float(flow['time_start'])\n",
        "            else:\n",
        "                tmp.append(0)\n",
        "            times = []\n",
        "            directions = []\n",
        "            pkt_sizes = []\n",
        "            ## Store in same place if using the same key\n",
        "            if key in data:\n",
        "              times = data[key][0]\n",
        "              directions = data[key][1]\n",
        "              pkt_sizes = data[key][2]\n",
        "            else:\n",
        "              times = []\n",
        "              directions = []\n",
        "              pkt_sizes = []\n",
        "              data[key] = []\n",
        "            total_time = 0\n",
        "            ## Store packet information \n",
        "            if flow['packets'] != []:\n",
        "              for packet in flow['packets']:\n",
        "                  total_time +=(float(packet['ipt']))/1000\n",
        "                  times.append(time_start + total_time+ (float(packet['ipt']))/1000)\n",
        "                  pkt_sizes.append(float(packet['b']))\n",
        "                  if packet['dir'] == '>':\n",
        "                      directions.append(1)\n",
        "                  else:\n",
        "                      directions.append(-1)\n",
        "            #Only get the first 64 packet information \n",
        "            data[key] = [times[:64],directions[:64],pkt_sizes[:64]]\n",
        "        return data\n",
        "              \n",
        "            \n",
        "         \n",
        "    ## Get flow statistics for the joy data \n",
        "    def getFlowSetMetadata(self):\n",
        "        if self.flows == []:\n",
        "            print(\"HERE\")\n",
        "            return None\n",
        "        data = dict()\n",
        "        for flow in self.flows:\n",
        "            if len(flow['packets']) == 0:\n",
        "                continue\n",
        "            tmp = []\n",
        "            reverse = False\n",
        "            if validate_ip(flow['sa']) == False or validate_ip(flow['da']) == False:\n",
        "              continue\n",
        "            key = flow['sa'].replace('.','')+flow['da'].replace('.','')+str(flow['sp'])+str(flow['dp'])+str(flow['pr'])\n",
        "            if reverse == False:    \n",
        "                if flow['dp'] != None:\n",
        "                    tmp.append(int(flow['dp']))  # destination port\n",
        "                else:\n",
        "                    tmp.append(0)  # ICMP/e\n",
        "                if flow['sp'] != None:\n",
        "                    tmp.append(int(flow['sp']))  # destination port\n",
        "                else:\n",
        "                    tmp.append(0)  # ICMP/etc.\n",
        "                if 'num_pkts_in' in flow:\n",
        "                    tmp.append(flow['num_pkts_in'])  # inbound packets\n",
        "                else:\n",
        "                    tmp.append(0)\n",
        "                if 'num_pkts_out' in flow:\n",
        "                    tmp.append(flow['num_pkts_out'])  # outbound packets\n",
        "                else:\n",
        "                    tmp.append(0)\n",
        "                if 'bytes_in' in flow:\n",
        "                    tmp.append(flow['bytes_in'])  # inbound bytes\n",
        "                else:\n",
        "                    tmp.append(0)\n",
        "                if 'bytes_out' in flow:\n",
        "                    tmp.append(flow['bytes_out'])  # outbound bytes\n",
        "                else:\n",
        "                    tmp.append(0)\n",
        "                if 'time_end' in flow:\n",
        "                    tmp.append(float(flow['time_end']))\n",
        "                else:\n",
        "                    tmp.append(0)\n",
        "\n",
        "                # elapsed time of flow\n",
        "                directions = []\n",
        "                pkt_dir_sizes =[]\n",
        "                if flow['packets'] == []:\n",
        "\n",
        "                    # # Interarrival Times\n",
        "                    tmp.append([])\n",
        "                    tmp.append([])\n",
        "                    tmp.append([])\n",
        "\n",
        "                    # #  Packet byte sizes\n",
        "                    tmp.append([])\n",
        "                    tmp.append([])\n",
        "                    tmp.append([])\n",
        "                else:\n",
        "                    ## Get timing information for both forwards and backwards\n",
        "                    times = []\n",
        "                    for_times = []\n",
        "                    bac_times = []\n",
        "                    pkt_sizes = []\n",
        "                    for_pkt_size = []\n",
        "                    bac_pkt_size = []\n",
        "                    for packet in flow['packets']:\n",
        "                        pkt_timing = packet['ipt']\n",
        "                        times.append(pkt_timing)\n",
        "                        pkt_sizes.append(packet['b'])\n",
        "                        if packet['dir'] == '>':\n",
        "                            for_times.append(pkt_timing)\n",
        "                            for_pkt_size.append(packet['b'])\n",
        "                            directions.append(1)\n",
        "                            pkt_dir_sizes.append(int(packet['b']))\n",
        "                        else:\n",
        "                            bac_times.append(pkt_timing)\n",
        "                            bac_pkt_size.append(packet['b'])\n",
        "                            directions.append(-1)\n",
        "                            pkt_dir_sizes.append(-1*int(packet['b']))   \n",
        "                    tmp.append(times)\n",
        "                    tmp.append(for_times)\n",
        "                    tmp.append(bac_times)\n",
        "                    tmp.append(pkt_sizes)\n",
        "                    tmp.append(for_pkt_size)\n",
        "                    tmp.append(bac_pkt_size)\n",
        "                if 'pr' in flow:\n",
        "                    tmp.append(flow['pr'])  # protocol\n",
        "                else:\n",
        "                    tmp.append(0)\n",
        "                # # Handle PPI Data\n",
        "                if 'ppi' in flow:\n",
        "                    ## Used mainly for the flag information\n",
        "                    if flow['ppi'] == []:\n",
        "                        # # BIDIR FLAGS ##\n",
        "                        tmp.append(0)\n",
        "                        tmp.append(0)\n",
        "                        tmp.append(0)\n",
        "                        tmp.append(0)\n",
        "\n",
        "                        # # FOR FLAGS ##\n",
        "                        tmp.append(0)\n",
        "                        tmp.append(0)\n",
        "                        tmp.append(0)\n",
        "                        tmp.append(0)\n",
        "\n",
        "                        # # BACK FLAGS ##\n",
        "                        tmp.append(0)\n",
        "                        tmp.append(0)\n",
        "                        tmp.append(0)\n",
        "                        tmp.append(0)\n",
        "                    else:\n",
        "                        psh_flag_count = 0\n",
        "                        ack_flag_count = 0\n",
        "                        syn_flag_count = 0\n",
        "                        fin_flag_count = 0\n",
        "\n",
        "                        for_psh_flag_count = 0\n",
        "                        for_ack_flag_count = 0\n",
        "                        for_syn_flag_count = 0\n",
        "                        for_fin_flag_count = 0\n",
        "\n",
        "                        bac_psh_flag_count = 0\n",
        "                        bac_ack_flag_count = 0\n",
        "                        bac_syn_flag_count = 0\n",
        "                        bac_fin_flag_count = 0\n",
        "                        for packetppi in flow['ppi']:\n",
        "                            if packetppi['dir'] == '>':\n",
        "                                if 'A' in packetppi['flags']:\n",
        "                                    ack_flag_count += 1\n",
        "                                    for_psh_flag_count += 1\n",
        "                                if 'S' in packetppi['flags']:\n",
        "                                    syn_flag_count += 1\n",
        "                                    for_ack_flag_count += 1\n",
        "                                if 'P' in packetppi['flags']:\n",
        "                                    syn_flag_count += 1\n",
        "                                    for_syn_flag_count += 1\n",
        "                                if 'F' in packetppi['flags']:\n",
        "                                    fin_flag_count += 1\n",
        "                                    for_fin_flag_count += 1\n",
        "                            else:\n",
        "                                if 'A' in packetppi['flags']:\n",
        "                                    ack_flag_count += 1\n",
        "                                    bac_psh_flag_count += 1\n",
        "                                if 'S' in packetppi['flags']:\n",
        "                                    syn_flag_count += 1\n",
        "                                    bac_ack_flag_count += 1\n",
        "                                if 'P' in packetppi['flags']:\n",
        "                                    syn_flag_count += 1\n",
        "                                    bac_syn_flag_count += 1\n",
        "                                if 'F' in packetppi['flags']:\n",
        "                                    fin_flag_count += 1\n",
        "                                    bac_fin_flag_count += 1\n",
        "                        tmp.append(psh_flag_count)\n",
        "                        tmp.append(ack_flag_count)\n",
        "                        tmp.append(syn_flag_count)\n",
        "                        tmp.append(fin_flag_count)\n",
        "\n",
        "                        tmp.append(for_psh_flag_count)\n",
        "                        tmp.append(for_ack_flag_count)\n",
        "                        tmp.append(for_syn_flag_count)\n",
        "                        tmp.append(for_fin_flag_count)\n",
        "\n",
        "                        tmp.append(bac_psh_flag_count)\n",
        "                        tmp.append(bac_ack_flag_count)\n",
        "                        tmp.append(bac_syn_flag_count)\n",
        "                        tmp.append(bac_fin_flag_count)\n",
        "                else:\n",
        "                    ## BIDIR FLAGS ##\n",
        "                    tmp.append(0)\n",
        "                    tmp.append(0)\n",
        "                    tmp.append(0)\n",
        "                    tmp.append(0)\n",
        "\n",
        "                    # # FOR FLAGS ##\n",
        "                    tmp.append(0)\n",
        "                    tmp.append(0)\n",
        "                    tmp.append(0)\n",
        "                    tmp.append(0)\n",
        "\n",
        "                    # # BACK FLAGS ##\n",
        "                    tmp.append(0)\n",
        "                    tmp.append(0)\n",
        "                    tmp.append(0)\n",
        "                    tmp.append(0)\n",
        "             \n",
        "            dns_ = [] ## Get DNS information \n",
        "            if 'dns' in flow:\n",
        "                for domain in flow['dns']:\n",
        "                    if 'qn' in domain:\n",
        "                        dns_.append(domain['qn'])\n",
        "            tmp.append(dns_)\n",
        "            tmp.append(directions)\n",
        "            tmp.append(pkt_dir_sizes)       \n",
        "            if 'time_start' in flow:\n",
        "                tmp.append(float(flow['time_start']))\n",
        "            if key not in data:\n",
        "                data[key] = [tmp]\n",
        "            else:\n",
        "                data[key].append(tmp)\n",
        "        if len(data) == 0:\n",
        "            return None\n",
        "        return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ASxqyCfcSTSB",
        "colab": {}
      },
      "source": [
        "import scipy.stats\n",
        "## Get individual stats for a given list \n",
        "def getStats(datax, storage):\n",
        "    data = [float(i) for i in datax]\n",
        "    if data != []:\n",
        "        storage.append(sum(data))\n",
        "        storage.append(min(data))\n",
        "        storage.append(max(data))\n",
        "        storage.append(np.average(data))\n",
        "        storage.append(np.var(data))\n",
        "    else:\n",
        "        for i in range(0,6):\n",
        "            storage.append(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_NfSNTI2xAZL",
        "colab": {}
      },
      "source": [
        "def extractFlowSetFeaturesLSTM(flowSetMetadata):\n",
        "    flowFeatureSet = dict()\n",
        "    \n",
        "    # # For each FlowSet get statistics\n",
        "    for key in flowSetMetadata:\n",
        "        ## Calculate Statistics for each \n",
        "        # # Calculate flowset features ##\n",
        "        # # Feature set includes minium, maximum, mean\n",
        "        # #  standard deviation, variance         ###\n",
        "        flowFeatureSet[key] = []\n",
        "        index = 0\n",
        "        prvious_time = 0\n",
        "        for flow in flowSetMetadata[key]:\n",
        "            # # flow[0]= dp\n",
        "            # # flow[1] = sp\n",
        "            # # flow[2] = num_pkts_in\n",
        "            # # flow[3] = num_pkts_out\n",
        "            # # flow[4] = bytes_in\n",
        "            # # flow[5] = bytes_out\n",
        "            # # flow[6] = time_end\n",
        "            # # flow[7] = bi_times\n",
        "            # # flow[8] = bac_times\n",
        "            # # flow[9] = for_times\n",
        "            # # flow[10] = pkt_sizes\n",
        "            # # flow[11] = for_pkt_size\n",
        "            # # flow[12] = bac_pkt_size\n",
        "            # # flow[13] = protocol\n",
        "            # # flow[14] = psh_flag_count\n",
        "            # # flow[15] = ack_flag_count\n",
        "            # # flow[16] = syn_flag_count\n",
        "            # # flow[17] = fin_flag_count\n",
        "            # # flow[18] = for_psh_flag_count\n",
        "            # # flow[19] = for_ack_flag_count\n",
        "            # # flow[20] = for_syn_flag_count\n",
        "            # # flow[21] = for_fin_flag_count\n",
        "            # # flow[22] = bac_psh_flag_count\n",
        "            # # flow[23] = bac_ack_flag_count\n",
        "            # # flow[24] = bac_syn_flag_count\n",
        "            # # flow[25] = bac_fin_flag_count\n",
        "            ##  flow[26] = DNS \n",
        "            flowFeatureSetCat = []\n",
        "            flowFeatureSetCont = []\n",
        "            flowFeatureSetBin = []\n",
        "            flow_dns = []\n",
        "            ## EXTRACT FEATURE SET FOR BACKWARD DIRECTION ##\n",
        "            flowFeatureSetCont.append(flow[2]) ## BACK NUMBER OF PKTS\n",
        "            flowFeatureSetCont.append(flow[4]) ## BACK BYTES IN \n",
        "            getStats(flow[8],flowFeatureSetCont) ## BAC TIME\n",
        "            getStats(flow[12],flowFeatureSetCont) ## BAC PKT SIZES\n",
        "            flowFeatureSetCont.append(flow[22]) ## PSH FLAG COUNT\n",
        "            flowFeatureSetCont.append(flow[23]) ## ACK FLAG COUNT\n",
        "            flowFeatureSetCont.append(flow[24]) ## SYN FLAG COUNT\n",
        "            flowFeatureSetCont.append(flow[25]) ## FIN FLAG COUNT\n",
        "            \n",
        "            ## EXTRACT FEATURE SET FOR FORWARD DIRECTION ##\n",
        "            flowFeatureSetCont.append(flow[3]) ## FORW NUMBER OF PKTS\n",
        "            flowFeatureSetCont.append(flow[5]) ## FORW BYTES IN \n",
        "            getStats(flow[9],flowFeatureSetCont) ## FORW TIME\n",
        "            getStats(flow[11],flowFeatureSetCont) ## FORW PKT SIZES\n",
        "            flowFeatureSetCont.append(flow[18]) ## PSH FLAG COUNT\n",
        "            flowFeatureSetCont.append(flow[19]) ## ACK FLAG COUNT\n",
        "            flowFeatureSetCont.append(flow[20]) ## SYN FLAG COUNT\n",
        "            flowFeatureSetCont.append(flow[21]) ## FIN FLAG COUNT\n",
        "            \n",
        "            ## EXTRACT FEATURE SET FOR BIDIRECTION  ##\n",
        "            flowFeatureSetCont.append(flow[3] +flow[2]) ## BID NUMBER OF PKTS\n",
        "            flowFeatureSetCont.append(flow[5] + flow[4]) ## BID BYTES IN \n",
        "            getStats(flow[7],flowFeatureSetCont) ## BID TIME\n",
        "            getStats(flow[10],flowFeatureSetCont) ## BID PKT SIZES\n",
        "            flowFeatureSetCont.append(flow[14]) ## PSH FLAG COUNT\n",
        "            flowFeatureSetCont.append(flow[15]) ## ACK FLAG COUNT\n",
        "            flowFeatureSetCont.append(flow[16]) ## SYN FLAG COUNT\n",
        "            flowFeatureSetCont.append(flow[17]) ## FIN FLAG COUNT\n",
        "            if index == 0:\n",
        "                flowFeatureSetCont.append(0)\n",
        "                previous_time = flow[6]\n",
        "            else:\n",
        "                flowFeatureSetCont.append(flow[6]-previous_time)\n",
        "            index+=1\n",
        "            ## Get Port Feature Set\n",
        "            flowFeatureSetCat.append(flow[0])\n",
        "            flowFeatureSetCat.append(flow[1])\n",
        "            \n",
        "            ## Get Protocol Feature Set\n",
        "            flowFeatureSetCat.append(flow[13])\n",
        "            \n",
        "            ## Get Adress Information\n",
        "            flowFeatureSetBin.append(flow[27])\n",
        "            flowFeatureSetBin.append(flow[28])\n",
        "            \n",
        "            flow_dns.append(flow[26])\n",
        "\n",
        "            flowFeatureSet[key].append(flowFeatureSetCat)\n",
        "            flowFeatureSet[key].append(flowFeatureSetCont)\n",
        "            flowFeatureSet[key].append(flowFeatureSetBin)\n",
        "            flowFeatureSet[key].append(flow_dns)\n",
        "    return flowFeatureSet\n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BmwjceiNSTSF",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from scipy import stats\n",
        "def extractFlowSetFeaturesCNN(flowSetMetadata):\n",
        "    flowFeatureSet = dict()\n",
        "\n",
        "    # # For each FlowSet get statistics\n",
        "    index = 0\n",
        "    key_counter = dict()\n",
        "    for key in flowSetMetadata:\n",
        "        # # Calculate Statistics for each \n",
        "        # # Calculate flowset features ##\n",
        "        # # Feature set includes minium, maximum, mean\n",
        "        # #  standard deviation, variance         ###\n",
        "        index += 1\n",
        "        key_counter[key] = 0 \n",
        "        # # for each flow in the flowset get the feature go through all \n",
        "        # # flows in the flowset\n",
        "        bi_number_of_pkts = []\n",
        "        bi_size = []\n",
        "        bac_number_of_pkts = []\n",
        "        bac_size = []\n",
        "        bac_time = []\n",
        "        for_number_of_pkts = []\n",
        "        for_size = []\n",
        "        for_time = []\n",
        "        number_of_flows = len(flowSetMetadata[key])\n",
        "        intertime = []\n",
        "        duration = []\n",
        "        pkt_times = []\n",
        "        protocols = []\n",
        "        previous_time = 0\n",
        "        flow_src_ports = []\n",
        "        flow_dest_ports = []\n",
        "        pkt_sizes = []\n",
        "        for_pkt_sizes = []\n",
        "        bac_pkt_sizes = []\n",
        "        psh_flag_count = []\n",
        "        ack_flag_count = []\n",
        "        syn_flag_count = []\n",
        "        fin_flag_count = []\n",
        "        for_psh_flag_count = []\n",
        "        for_ack_flag_count = []\n",
        "        for_syn_flag_count = []\n",
        "        for_fin_flag_count = []\n",
        "        bac_psh_flag_count = []\n",
        "        bac_ack_flag_count = []\n",
        "        bac_syn_flag_count = []\n",
        "        bac_fin_flag_count = []\n",
        "        dns =[]\n",
        "        subnet1 = []\n",
        "        subnet2 = []\n",
        "        directions = []\n",
        "        pkt_dir_sizes =[]\n",
        "        for flow in flowSetMetadata[key]:\n",
        "            # # flow[0]= dp\n",
        "            # # flow[1] = sp\n",
        "            # # flow[2] = num_pkts_in\n",
        "            # # flow[3] = num_pkts_out\n",
        "            # # flow[4] = bytes_in\n",
        "            # # flow[5] = bytes_out\n",
        "            # # flow[6] = time_end\n",
        "            # # flow[7] = bi_times\n",
        "            # # flow[8] = bac_times\n",
        "            # # flow[9] = for_times\n",
        "            # # flow[10] = pkt_sizes\n",
        "            # # flow[11] = for_pkt_size\n",
        "            # # flow[12] = bac_pkt_size\n",
        "            # # flow[13] = protocol\n",
        "            # # flow[14] = psh_flag_count\n",
        "            # # flow[15] = ack_flag_count\n",
        "            # # flow[16] = syn_flag_count\n",
        "            # # flow[17] = fin_flag_count\n",
        "            # # flow[18] = for_psh_flag_count\n",
        "            # # flow[19] = for_ack_flag_count\n",
        "            # # flow[20] = for_syn_flag_count\n",
        "            # # flow[21] = for_fin_flag_count\n",
        "            # # flow[22] = bac_psh_flag_count\n",
        "            # # flow[23] = bac_ack_flag_count\n",
        "            # # flow[24] = bac_syn_flag_count\n",
        "            # # flow[25] = bac_fin_flag_count\n",
        "            # # flow[29] = directions\n",
        "            flow_dest_ports.append(flow[0])\n",
        "            flow_src_ports.append(flow[1])\n",
        "\n",
        "            # ##       BACKWARD      ###\n",
        "            # # NUMBER ##\n",
        "            # # Get num_pkts_in##\n",
        "            bac_number_of_pkts.append(flow[2])\n",
        "\n",
        "            # # SIZE ##\n",
        "            # # Get bytes_in##\n",
        "            bac_size.append(flow[4])\n",
        "\n",
        "            # #  interarrival times ##\n",
        "            bac_time = bac_time + flow[8]\n",
        "\n",
        "            # # bac pkt size ##\n",
        "            bac_pkt_sizes = bac_pkt_sizes + flow[12]\n",
        "\n",
        "            # # psh_flag_count\n",
        "            bac_psh_flag_count.append(flow[22]) \n",
        "\n",
        "            # # ack_flag_count\n",
        "            bac_ack_flag_count.append(flow[23])\n",
        "\n",
        "            # # syn_flag_count\n",
        "            bac_syn_flag_count.append(flow[24])\n",
        "\n",
        "            # # fin_flag_count\n",
        "            bac_fin_flag_count.append(flow[25])\n",
        "\n",
        "            # ##       FORWARD      ###\n",
        "            # # NUMBER ##\n",
        "            # # Get num_pkts_out##\n",
        "            for_number_of_pkts.append(flow[3])\n",
        "\n",
        "            # # SIZE ##\n",
        "            # # Get bytes_out##\n",
        "            for_size.append(flow[5])\n",
        "\n",
        "            # #  interarrival times ##\n",
        "            for_time = for_time + flow[9]\n",
        "\n",
        "            # # for pkt size ##\n",
        "            for_pkt_sizes = for_pkt_sizes + flow[11]\n",
        "\n",
        "            # # psh_flag_count\n",
        "            for_psh_flag_count.append(flow[18])\n",
        "\n",
        "            # # ack_flag_count\n",
        "            for_ack_flag_count.append(flow[19])\n",
        "\n",
        "            # # syn_flag_count\n",
        "            for_syn_flag_count.append(flow[20])\n",
        "\n",
        "            # # fin_flag_count\n",
        "            for_fin_flag_count.append(flow[21])\n",
        "\n",
        "            # ##      BIDIRECTION         ####\n",
        "            # # NUMBER ##\n",
        "            bi_number_of_pkts.append(flow[2] + flow[3])\n",
        "\n",
        "            # # SIZE ##\n",
        "            bac_number_of_pkts.append(flow[4] + flow[5])\n",
        "\n",
        "            ## INTER FLOW TIME ##\n",
        "            if previous_time != 0:\n",
        "                intertime.append(flow[6] - previous_time)\n",
        "            previous_time = flow[6]\n",
        "            duration.append(flow[6]-flow[len(flow)-1])\n",
        "\n",
        "            ## Inter arrival times ##\n",
        "            pkt_times = pkt_times + flow[7]\n",
        "\n",
        "            ## pkt sizes ##\n",
        "            pkt_sizes = pkt_sizes + flow[10]\n",
        "\n",
        "            ## psh_flag_count\n",
        "            psh_flag_count.append(flow[14])\n",
        "\n",
        "            ## ack_flag_count\n",
        "            ack_flag_count.append(flow[15])\n",
        "\n",
        "            ## syn_flag_count\n",
        "            syn_flag_count.append(flow[16])\n",
        "\n",
        "            ## fin_flag_count\n",
        "            fin_flag_count.append(flow[17])\n",
        "            if flow[26] != []:\n",
        "                dns.append(flow[26])\n",
        "            protocols.append(flow[13])\n",
        "            subnet1.append(flow[27])\n",
        "            subnet2.append(flow[28])\n",
        "            directions = directions +flow[29]\n",
        "            pkt_dir_sizes = pkt_dir_sizes +flow[30]\n",
        "        if dns == []:\n",
        "            dns.append(\"\")\n",
        "        # # Calculate flowset features ##\n",
        "        # # Feature set includes minium, maximum, mean, mean absolute devation,\n",
        "        # # kurtosis, skewness, standard deviation, variance, and quantiles ###\n",
        "\n",
        "        flowFeatureSet[key] = []\n",
        "        flowFeatureSetCat = []\n",
        "        flowFeatureSetCont = []\n",
        "        flowFeatureSetBin = []\n",
        "\n",
        "        # ## EXTRACT FEATURE SET FOR BACKWARD DIRECTION ##\n",
        "        # # NUM PACKETS FEAUTRES ###\n",
        "        getStats(bac_number_of_pkts, flowFeatureSetCont)\n",
        "\n",
        "        # # BYTE SIZE FEATUREs ###\n",
        "        getStats(bac_size, flowFeatureSetCont)\n",
        "\n",
        "        # # DURATION FEATURES ##\n",
        "        getStats(bac_time, flowFeatureSetCont)\n",
        "        \n",
        "        #flowFeatureSetCont.append(len(np.unique(flow_dest_ports)))\n",
        "        #flowFeatureSetCont.append(stats.mode(flow_dest_ports,axis=None)[0])\n",
        "        flowFeatureSetCont.append(stats.mode(flow_dest_ports,axis=None)[1].astype(int))\n",
        "\n",
        "        # ## EXTRACT FEATURE SET FOR FORWARD DIRECTION ##\n",
        "        # # NUM PACKETS FEAUTRES ###\n",
        "        getStats(for_number_of_pkts, flowFeatureSetCont)\n",
        "\n",
        "        # # BYTE SIZE FEATUREs ###\n",
        "        getStats(for_size, flowFeatureSetCont)\n",
        "\n",
        "        # # DURATION FEATURES ##\n",
        "        getStats(for_time, flowFeatureSetCont)\n",
        "        flowFeatureSetCont.append(stats.mode(flow_src_ports,axis=None)[1].astype(float))\n",
        "\n",
        "        # ## EXTRACT FEATURE SET FOR BIDIRECTION ##\n",
        "        # # NUM PACKETS FEAUTRES ###\n",
        "        getStats(bi_number_of_pkts, flowFeatureSetCont)\n",
        "        \n",
        "        # # BYTE SIZE FEATUREs ###\n",
        "        getStats(bi_size, flowFeatureSetCont)\n",
        "        \n",
        "         # # PACKT INTERARRIVAL TIMES ##\n",
        "        getStats(pkt_times, flowFeatureSetCont)\n",
        "\n",
        "\n",
        "        # # INTER ARRIVAL TIME SIZE FEATURES ###\n",
        "        getStats(intertime, flowFeatureSetCont)\n",
        "\n",
        "        # # DURATION FEATUREs ###\n",
        "        getStats(duration, flowFeatureSetCont)\n",
        "\n",
        "       \n",
        "        # # PACKET SIZES ##\n",
        "        getStats(pkt_sizes, flowFeatureSetCont)\n",
        "\n",
        "        # # FOR PACKET SIZES ##\n",
        "        getStats(for_pkt_sizes, flowFeatureSetCont)\n",
        "\n",
        "        # # BAC PACKET SIZES ##\n",
        "        getStats(bac_pkt_sizes, flowFeatureSetCont)\n",
        "\n",
        "        # # PSH FLAG  ##\n",
        "        getStats(psh_flag_count, flowFeatureSetCont)\n",
        "\n",
        "        # # ACK FLAG  ##\n",
        "        getStats(ack_flag_count, flowFeatureSetCont)\n",
        "\n",
        "        # # SYN FLAG  ##\n",
        "        getStats(syn_flag_count, flowFeatureSetCont)\n",
        "\n",
        "        # # FIN FLAG  ##\n",
        "        getStats(fin_flag_count, flowFeatureSetCont)\n",
        "\n",
        "        # # FOR PSH FLAG  ##\n",
        "        getStats(for_psh_flag_count, flowFeatureSetCont)\n",
        "\n",
        "        # # ACK FLAG  ##\n",
        "        getStats(for_ack_flag_count, flowFeatureSetCont)\n",
        "\n",
        "        # # SYN FLAG  ##\n",
        "        getStats(for_syn_flag_count, flowFeatureSetCont)\n",
        "\n",
        "        # # FIN FLAG  ##\n",
        "        getStats(for_fin_flag_count, flowFeatureSetCont)\n",
        "\n",
        "        ## BAC PSH FLAG  ##\n",
        "        getStats(bac_psh_flag_count, flowFeatureSetCont)\n",
        "\n",
        "        # # ACK FLAG  ##\n",
        "        getStats(bac_ack_flag_count, flowFeatureSetCont)\n",
        "\n",
        "        # # SYN FLAG  ##\n",
        "        getStats(bac_syn_flag_count, flowFeatureSetCont)\n",
        "\n",
        "        # # FIN FLAG  ##\n",
        "        getStats(bac_fin_flag_count, flowFeatureSetCont)\n",
        "\n",
        "        # # Get Protocol Features  ##\n",
        "        #for protocol in protocols:\n",
        "        flowFeatureSetCat.append(protocols)\n",
        "        ##for sub in subnet1:\n",
        "        flowFeatureSetBin.append(subnet1)\n",
        "        #for sub in subnet2:\n",
        "        flowFeatureSetBin.append(subnet2)\n",
        "        \n",
        "        ## Get Port values \n",
        "        flowFeatureSetCat.append(stats.mode(flow_src_ports,axis=None)[0])\n",
        "        flowFeatureSetCat.append(stats.mode(flow_dest_ports,axis=None)[0])\n",
        "        \n",
        "        ## Append all values for a given key  \n",
        "        flowFeatureSet[key].append(flowFeatureSetCat)\n",
        "        flowFeatureSet[key].append(flowFeatureSetCont)\n",
        "        flowFeatureSet[key].append(flowFeatureSetBin)\n",
        "        flowFeatureSet[key].append(pkt_times)\n",
        "        flowFeatureSet[key].append(directions)\n",
        "        flowFeatureSet[key].append(pkt_dir_sizes)\n",
        "        flowFeatureSet[key].append(dns)\n",
        "    return flowFeatureSet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uhGrKchhSTSI",
        "colab": {}
      },
      "source": [
        "import os\n",
        "## Get the malware and benign files \n",
        "malware_files = []\n",
        "benign_files = []\n",
        "for file in os.listdir('./datasets/malware_json/'):\n",
        "    filename = os.fsdecode(file)\n",
        "    if filename.endswith('.json'):\n",
        "        malware_files.append('./datasets/malware_json/' + filename)\n",
        "    else:\n",
        "        continue\n",
        "for file in os.listdir('./datasets/benign_json'):\n",
        "    filename = os.fsdecode(file)\n",
        "    if filename.endswith('.json'):\n",
        "        benign_files.append('./datasets/benign_json/' + filename)\n",
        "    else:\n",
        "        continue"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ca4WjyEr3MwX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "import gc\n",
        "malign_flows = []\n",
        "## Get the metadata for the given malign flows \n",
        "for malware_file in malware_files:\n",
        "    data_parser = DataParser(malware_file)\n",
        "    mal_flowSetMetadata = data_parser.getFlowSetMetadata()\n",
        "    if mal_flowSetMetadata != None:\n",
        "      malign_flows.append(extractFlowSetFeaturesCNN(mal_flowSetMetadata))\n",
        "    del mal_flowSetMetadata\n",
        "    gc.collect()\n",
        "## Dump the the embedding data (Change for given configuration)\n",
        "with open('./embedding/CNN_FULL/metadata_mal_encodings_CNN', 'wb') as fp:\n",
        "      pickle.dump(malign_flows, fp)\n",
        "      fp.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GG934tfvxkgY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "import gc\n",
        "malign_flows = []\n",
        "## Get the metadata for the given malign flows \n",
        "for malware_file in malware_files:\n",
        "    data_parser = DataParser(malware_file)\n",
        "    mal_flowSetMetadata = data_parser.getFlowSetMetadataTimings()\n",
        "    malign_flows.append(mal_flowSetMetadata)\n",
        "    del mal_flowSetMetadata\n",
        "    gc.collect()\n",
        "## Dump the the timing data (Change for given configuration)\n",
        "with open('./embedding/CNN_FULL/time_mal_encodings_CNN', 'wb') as fp:\n",
        "      pickle.dump(malign_flows, fp)\n",
        "      fp.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hm1BnBOXH7Kw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "import gc\n",
        "benign_flows = []\n",
        "## Get the metadata for the given benign flows \n",
        "for benign_file  in benign_files:\n",
        "    data_parser = DataParser(malware_file)\n",
        "    benign_flowSetMetadata = data_parser.getFlowSetMetadataTimings()\n",
        "    benign_flows.append(benign_flowSetMetadata)\n",
        "    del benign_flowSetMetadata\n",
        "    gc.collect()\n",
        "## Dump the the timing data (Change for given configuration)\n",
        "with open('./embedding/CNN_FULL/time_benign_encodings_CNN', 'wb') as fp:\n",
        "      pickle.dump(malign_flows, fp)\n",
        "      fp.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hs5SXV7LybHe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('./embedding/CNN_FULL/time_mal_encodings_CNN', 'rb') as fp:\n",
        "    malign_flows = pickle.load(fp)\n",
        "    fp.close()\n",
        "mal_flows = []\n",
        "\n",
        "## Handles soring the flows accroding to the time in which the occured rather than \n",
        "## in the way that joy returned the flows\n",
        "for file in malign_flows:\n",
        "  flowsets= dict()\n",
        "  for key in file:\n",
        "    sorted_timings = sorted(file[key][0])\n",
        "    sorted_directions = [x for _,x in sorted(zip(file[key][0],file[key][1]))]\n",
        "    sorted_pkt_times = [x for _,x in sorted(zip(file[key][0],file[key][2]))]\n",
        "    flowsets[key] = [sorted_timings,sorted_directions,sorted_pkt_times]\n",
        "  mal_flows.append(flowsets)\n",
        "with open('./embedding/CNN_FULL/time_mal_encodings_CNN', 'wb') as fp:\n",
        "    pickle.dump(mal_flows, fp)\n",
        "    fp.close()\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCp0FwpgJVjx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('./embedding/CNN_FULL/time_benign_encodings_CNN', 'rb') as fp:\n",
        "    benign = pickle.load(fp)\n",
        "    fp.close()\n",
        "mal_flows = []\n",
        "## Handles soring the flows accroding to the time in which the occured rather than \n",
        "## in the way that joy returned the flows\n",
        "for file in benign_flows:\n",
        "  flowsets= dict()\n",
        "  for key in file:\n",
        "    sorted_timings = sorted(file[key][0])\n",
        "    sorted_directions = [x for _,x in sorted(zip(file[key][0],file[key][1]))]\n",
        "    sorted_pkt_times = [x for _,x in sorted(zip(file[key][0],file[key][2]))]\n",
        "    flowsets[key] = [sorted_timings,sorted_directions,sorted_pkt_times]\n",
        "  benign_flows.append(flowsets)\n",
        "with open('./embedding/CNN_FULL/time_benign_encodings_CNN', 'wb') as fp:\n",
        "    pickle.dump(benign_flows, fp)\n",
        "    fp.close()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}